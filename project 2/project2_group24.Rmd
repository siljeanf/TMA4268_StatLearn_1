
---
subtitle: "TMA4268 Statistical Learning V2019"
title: "Compulsory exercise 2: Group 24"
author: "Silje Anfindsen and Clara Panchaud"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
 # html_document
 pdf_document
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=68),tidy=TRUE)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize")
```


# Problem 1

## a)
Let's find the ridge regression estimator. Remember that $\hat{\beta}_{Ridge}$ minimizes $RSS+\lambda\sum_{j=1}^p\beta_j^2$. Let's rewrite this in matrix notation.

$$
\begin{aligned}
min_{\beta}\{(y-X\beta)^T(y-X\beta)+\lambda \beta^T\beta\}&=  &&\text{develop the expression}\\ 
min_{\beta}\{y^Ty-2\beta^TX^Ty+\beta^TX^TX\beta+\lambda \beta^T\beta\}&  &&\text{take the derivative with respect to beta and set equal to 0}\\
-2X^Ty+2X^TX\beta +2\lambda \beta=0 \\
(X^TX+2\lambda I)\beta=X^Ty\\
\beta = (X^TX+\lambda I)^{-1}X^Ty
\end{aligned}
$$
Therefore the estimator is $\hat{\beta}_{Ridge}=(X^TX+\lambda I)^{-1}X^Ty$.

## b)


To find the expected value and the variance-covariance matrix of $\hat{\beta}_{Ridge}$ we need to remember the distribution of y, $y\sim N(X\beta,\sigma^2I)$. Therefore we get the expected value: 

$$E(\hat{\beta}_{Ridge})=E((X^TX+\lambda I)^{-1}X^Ty)=(X^TX+\lambda I)^{-1}X^TE(y)=(X^TX+\lambda I)^{-1}X^TX\beta$$
and the variance-covariance matrix:

$$
\begin{aligned}
Var(\hat{\beta}_{Ridge})=
Var((X^TX+\lambda I)^{-1}X^Ty)&=&&\text{by property of the variance}\\
(X^TX+\lambda I)^{-1}X^TVar(y)((X^TX+\lambda I)^{-1}X^T)^T&= &&\text{develop the expression}\\
\sigma^2(X^TX+\lambda I)^{-1}X^TX(X^TX+\lambda I)^{-1}  \\
\end{aligned}
$$


## c)

TRUE, FALSE, FALSE, TRUE

## d)
```{r, echo=TRUE, eval=TRUE}
library(ISLR)
library(leaps)
library(glmnet)
```

We want to work with the $College$ data. First we split it into a training and a testing set.
```{r, echo=TRUE, eval=TRUE}
set.seed(1)

#make training and testing set 
train.ind = sample(1:nrow(College), 0.5 * nrow(College))
college.train = College[train.ind, ]
college.test = College[-train.ind, ]

#the structure of the data 
str(College)
```

Now we will apply forward selection, using $Outstate$ as a response. We have 18 variables including the response so we will obtain a model including up to 17 variables.
```{r, echo=TRUE, eval=TRUE}
nb_predictors<-17
forward<-regsubsets(Outstate~.,college.train,nvmax=17,method="forward")
sum<-summary(forward)
```

In Figure \ref{fig:fig1} we can look at the RSS and the adjusted $R^2$ in order to pick the number of variables that gives the optimal result. Remember that if the difference is not very significant we would rather pick the simplest model. It seems like 5 variables would be good here.

```{r, echo=TRUE, eval=TRUE, fig1,fig.cap="Comparison of models with different number of variables. \\label{fig:fig1}"}
par(mfrow=c(1,2))
plot(sum$rss,xlab="Number of Variables",ylab="RSS",type="l")
plot(sum$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
```

Below are the chosen variables when we decide to include 5 variables in the reduced model.
```{r, echo=TRUE, eval=TRUE}

nb_selected_pred<-5
variables<-names( coef( forward,id=nb_selected_pred ) )
variables
```

We will now find the reduced model as well as the MSE (mean squared error) on the test set.

```{r, echo=TRUE, eval=TRUE}
#fit the reduced model
reduced.model<-lm(Outstate~Private+Room.Board+Grad.Rate+perc.alumni+Expend, data =college.train)
summary(reduced.model)
```

The reduced model is
$$ \begin{aligned}
\textbf{Outstate} &= `r reduced.model$coefficients[1]` + `r reduced.model$coefficients[2]` \textbf{Private}+`r reduced.model$coefficients[3]` \textbf{Room.Board}\\
&+ `r reduced.model$coefficients[4]` \textbf{Grad.Rate}+ `r reduced.model$coefficients[5]`  \textbf{perc.alumni} +  `r reduced.model$coefficients[6]`  \textbf{Expend}, 
\end{aligned}$$

```{r, echo=TRUE, eval=TRUE}
#find test MSE
p<-predict(reduced.model,newdata=college.test)
mse_fwd <- mean(((college.test$Outstate)-p)^2)
mse_fwd
```
The test MSE is
$$MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{f(x_i)})^2 =  `r mse_fwd`$$

## e)

We will now select a model for the same dataset as in (d) but this time with the Lasso method. Again, we use both a training and testing set for the data.

```{r, echo=TRUE, eval=TRUE}
#Make a x matrix and y vector for both the training and testing set
x_train<-model.matrix(Outstate~.,college.train)[,-1]
y_train<-college.train$Outstate
x_test<-model.matrix(Outstate~.,college.test)[,-1]
y_test<-college.test$Outstate

```

In order to select the best value for the tuning parameter $\lambda$ we will use cross validation.

```{r, echo=TRUE, eval=TRUE}
set.seed(5555)

#perform the Lasso method and choose the best model using CV
lasso.mod = glmnet(x_train,y_train,alpha=1) #lasso method on train set
cv.lasso = cv.glmnet(x_train,y_train,alpha=1) #CV on train set
lambda.best = cv.lasso$lambda.min #select best lambda
lambda.best

#find the test MSE
predictions<-predict(lasso.mod,s=lambda.best,newx=x_test)
mse_lasso <- mean((predictions-y_test)^2) #test MSE
mse_lasso
```

%%Check if log scale!!
From cross validation we can observe that the optimal tuning parameter is $\lambda = `r lambda.best`$ as this is the parameter that minimizes the MSE for the training set. 

The test MSE is now `r mse_lasso`, which is lower than what we found for the reduced model using forward selection in d).

The lasso yields sparse models which involves only a subset of variables. Lasso performs variable selection by forcing some of the coefficient estimates to be exactly zero. The selected variables that was not put to zero are displayed below.

```{r, echo=TRUE, eval=TRUE}
c<-coef(lasso.mod,s=lambda.best,exact=TRUE)
inds<-which(c!=0)
variables<-row.names(c)[inds]
variables
```


# Problem 2

## a)

FALSE, FALSE, TRUE, FALSE

## b) 

The basis functions for a cubic spline with knots at each quartile, of variable $X$ are,

$$
\begin{aligned}
&b_0(X) = 1 && b_4(X) = (X-q_1)_{+}^3 \\
&b_1(X) = x && b_5(x) = (X-q_2)_{+}^3 \\
&b_2(X) = x^2 && b_6(X) = (X-q_3)_{+}^3 \\
&b_3(X)=x^3
\end{aligned}
$$

## c)

We will now investigate the realtionship between $Outstate$ and the 6 of the predictors, $Private$, $Room.Board$, $Terminal$, $perc.alumni$, $Expend$, and $Grad.Rate$. 

```{r, echo=TRUE, eval=TRUE}

ds1 = college.train[c("Private", "Outstate")] #binary variable
ds2 = college.train[c("Room.Board", "Outstate")]
ds3 = college.train[c("Terminal", "Outstate")]
ds4 = college.train[c("perc.alumni", "Outstate")]
ds5 = college.train[c("Expend", "Outstate")]
ds6 = college.train[c("Grad.Rate", "Outstate")]

par(mfrow=c(2,3))
plot(ds1)
plot(ds2)
plot(ds3)
plot(ds4)
plot(ds5)
plot(ds6)
```

From each of the plots above we can conclude that at least $Terminal$ and $Expend$ seems to have a non-linear relationship with $Outstate$. These two variables therefore might benefit from a non-linear transformation. The others variables, $Room.Board$, $perc.alumni$ and $Grad.Rate$ seem to have a linear realtionship with the response variable. The binary variable $Private$ is presented through a boxplot. Generally the data seem to cathegorize quite well into these two classes of private and public universites where the trend is a higher out-of-state tuition for private universites, except for some outliers for public universities where the outcome is very high and therefore can seem to belong to a private universities. Anyway, we cannot transform a binary variable. 

## d)
### (i)
We will now fit several polynomial regression models for $Outstate$ with $Terminal$ as the only covariate. Each polynomial will have a degree from $d=1,..10$. 

```{r, echo=TRUE, eval=TRUE}
library(ggplot2)

#make a dataframe
ds = College[c("Terminal", "Outstate")]
n = nrow(ds)

# chosen degrees
deg = 1:10

#now iterate over each degree d
dat = c()  #make a empty variable to store predicted values for each degree
MSE_train_poly = c(rep(0,10))  #make a empty variable to store MSE for each degree
MSE_test_poly = c(rep(0,10))

for (d in deg) {
    # fit model with this degree
    mod = lm(Outstate ~ poly(Terminal, d), data= college.train)
    
    #dataframe for Terminal and Outstate showing result for each degree over all samples
    dat = rbind(dat, data.frame(Terminal = college.train$Terminal, Outstate = mod$fit, 
                                degree = as.factor(rep(d,length(mod$fit)))))
    # training MSE
    MSE_train_poly[d] = mean((predict(mod, newdata=college.train) - college.train$Outstate)^2)
    MSE_test_poly[d] = mean((predict(mod, newdata= college.test) - college.test$Outstate)^2)
}

# plot fitted values for different degrees
ggplot(data = ds[train.ind, ], aes(x = Terminal, y = Outstate)) + 
    geom_point(color = "darkgrey") + labs(title = "Polynomial regression")+
    geom_line(data = dat, aes(x = Terminal, y = Outstate, color = degree))

```

### (ii)
We will now choice a suitable smoothing spline model to predict $Outstate$ as a function of $Expend$ and plot the fitted function.

```{r, echo=TRUE, eval=TRUE}
library(splines)

#plot training set for Expend as only covariate
plot(college.train$Expend, college.train$Outstate, col = "darkgrey", 
     main="Smoothing spline", xlab="Expend", ylab="Outstate")

#perform CV in order to find optimal number of df
fit = smooth.spline(college.train$Expend, college.train$Outstate, cv=TRUE)
df <- fit$df #choose df from CV
l <- fit$lambda

#add fitted function from smoothing spline
lines(fit, col="red", lwd=2)
legend("topright", legend=c("4.6 DF"), col="red", lty=1, lwd=2, cex=.8)

#training MSE
pred = predict(fit, newdata=college.train)
MSE_spline_train = mean( (college.train$Outstate - pred$y)^2 )

#test MSE
pred = predict(fit, newdata=college.test)
MSE_spline_test = mean( (college.test$Outstate - pred$y)^2 )
```

In order to choose a suitable model we did cross validation. The optimal number of degrees of freedom is $df = `r df`$, which gives the smoothing parameter $\lambda = `r l`$

### (iii)
We will now report the training MSE for the polynomial regresson models and the smoothing spline model.
For the polynomial regression models we present the MSE for each degree of polynomial $d=1..,10$. 

```{r, echo=TRUE, eval=TRUE}
#MSE for polynomial regression models (1-10) 
plot(1:10,MSE_train_poly, type = "o", pch = 16, xlab = "degree", main = "Test error")
MSE_poly_train_best = MSE_spline_train[9]

#training MSE for smoothing spline
MSE_spline_test

```
For the polynomial regression we did expect the MSE to decrease for larger degree of polynomial untill a certain point. Here it seems that this point is located at degree $d=9$. 


(discuss is it is as expected!)

# Problem 3

## a)
FALSE, FALSE, TRUE, FALSE

## b)

To predict $Outstate$ we will choose 

PROS and CONS!
One known disadvantage of regression trees are that they usually have a high variance, and are not very robust. The idea of bagging is to grow many trees to fix those two problems. Bagging is a special case of another approach called random forrests. In random forrests we allow only certain number of predictors $m$ to be selected at each node, which deacreases the correlation between each tree. 

The tuning parameter in random forrests is the number of predictors you are able to choose between at each split. It is recommended to use $m=p/3$ variables when building a regression trees. Recall that we have $17$ parameters in our dataset. We will therefore pick $m=5$.

```{r, echo=TRUE, eval=TRUE}
library(randomForest)

set.seed(1)
bag.college <- randomForest(Outstate ~ .,data=college.train, mtry=5, ntrees=500)
yhat.college <- predict(bag.college, newdata=college.test)
MSE_rf <- mean((yhat.college - college.test$Outstate)^2)
MSE_rf

```

The test MSE for the random forests is $MSE_{rf} = `r MSE_rf`$.

## c)

We will now compare the test MSEs among the methods used on the data set $College$ so far. That is: the two linear model selection methods, forward selection and Lasso method, non-linear methods, polynomial regression and smoothing splines and at last the tree-based method Random forrests of regression trees. For the polynomial regression we just show the polynomial degree with the minimal test MSE.

```{r, echo=TRUE, eval=TRUE}

MSE <- c(mse_fwd,mse_lasso,min(MSE_test_poly),MSE_spline_test,MSE_rf)
Method <- c("Forward selection", "Lasso", "Polynomial regression", "Smoothing spline", 
            "Random Forest")
df <- data.frame(Method, MSE)
kable(df)
```

The method performing best in terms of prediction error is the random forest of regression trees. But if the aim is to develop a interpretable model Lasso is the best choice.


# Problem 4

Start by loading the data of $diabetes$ from a population of women.

```{r, echo=TRUE, eval=TRUE}
id <- "1Fv6xwKLSZHldRAC1MrcK2mzdOYnbgv0E"  # google file ID
d.diabetes <- dget(sprintf("https://docs.google.com/uc?id=%s&export=download", 
    id))
d.train = d.diabetes$ctrain
d.test = d.diabetes$ctest
```

## a)
In order to answer on the Mulitple choice we will have a look at the training data.

```{r, echo=TRUE, eval=TRUE}
library(GGally)
library(gridExtra)

max(d.train$npreg)#max nr of pregnancies
head(d.train) #overview of data

#plots
ggpairs(d.train) + theme_minimal() #look at correlation between variables
#plot2 <- ggplot(d.train, aes(x=npreg, y=diabetes))+geom_point()
plot2 <- ggplot(d.train, aes(npreg)) + geom_histogram(binwidth = 2) + labs(title = "Histogram") + 
    theme_minimal()
plot3 <- ggplot(d.train,aes(x=glu,y=bmi,color=diabetes))+geom_point()
grid.arrange(plot2, plot3, ncol=2, nrow = 1)

```


TRUE, TRUE, TRUE, TRUE

## b)

We will now fit a support vector classifier with linear boundary and a support vector machine with radial boundary to find good functions that predict the diabetes status of a patient. 

```{r, echo=TRUE, eval=TRUE}
library(e1071)

set.seed(10111)

d.train$diabetes <- as.factor(d.train$diabetes)
d.test$diabetes <- as.factor(d.test$diabetes)


#Fit a support vector classifier (linear boundary) 
svm.linear = svm(diabetes~., 
                 data = d.train, 
                 kernel = 'linear')

#fit a support vector machine (radial boundary)
svm.radial = svm(diabetes~., 
                 data = d.train, 
                 kernel = 'radial')

#CV to find the best parameters for each model
cv.linear <- tune(svm, diabetes ~ ., data=d.train, kernel = "linear", 
                  ranges = list(cost = c(.001, .01 , .1, 1, 10, 100 ) ) )

cv.radial <- tune(svm, diabetes ~., data = d.train, kernel = "radial",
                  ranges = list(cost = c(0.1,1,10,100,1000), gamma = c(0.5,1,2,3,4) ))

#fit new models with the optimalized parameters from CV
bestmod.linear = cv.linear$best.model 
bestmod.radial = cv.radial$best.model 

# Predict the response for the test set  
pred.lin = predict(bestmod.linear, newdata = d.test)
pred.rad = predict(bestmod.radial, newdata = d.test)
# Confusion tables (0: no diabetes, 1: diabetes) 
#for SVC (linear)
table(Prediction = pred.lin, Truth = d.test$diabetes)

#for SVM (radial)
table(Prediction = pred.rad, Truth = d.test$diabetes)
```


From the two confusion tables above we can find the missclassification rates. The left rate belongs to the support vector classifier (linear boundary), and the right rate to the support vector machine (radial boundary). 

$$
\begin{aligned}
\frac{35+18}{137+35+18+42} &= 0.2284 & \frac{38+22}{133+38+22+39} &= 0.2586
\end{aligned}
$$


We know that the missclassification error rate is a good performance measure for classification rules.
So by looking at these rates the support vector classifier with a linear boundary seems to make the best classification rule for the prescence of diabetes for our data. This is because the number of missclassifications is remarkable lower for this classifier. We also observe that the number of false positive and false negative findings are both lower for this chosen model compared to the support vector machine with radial boundary.


## c)

We will now compare the performance of the support vector classifier and support vector machine with a new method: logistic regression. As we have observed above, our data seems to performe well when being classified with a linear decision boundary. Logistic regression models the probability that the response belongs to one of the two classes, producing a linear decision boundary. Therefore this method seems to be a good choice for our data. 

```{r, echo=TRUE, eval=TRUE}

#fit a logistic regression model using training data
glm.fit <- glm(diabetes ~ .,data=d.train, family="binomial")

#predict the response using testing data
glm.probs <- predict(glm.fit, newdata=d.test, type="response")

#sort the probabilities for whether the observations are < or > than p = 0.5
glm.pred = rep("0",length(d.test$diabetes)) #create vector of nr. of elements = dataset
glm.pred[glm.probs>0.5]="1"

#confusion table
table(Prediction = glm.pred, Truth = d.test$diabetes)

```

The missclassification rate for logistic regression is 
$$
\frac{32+19}{136+45+32+19} = 0.2198 
$$
We notice that the missclassification rate for logistic regression is even lower than for the two SVMs. This indicates that logistic regression could be a even better choice in order to predict the prescense of diabetes based on our data. 

Both logistic regression and support vector machines solve classification problems. In logistic regression all observations contribute to the decision boundary, while for SVMs, only the support vectors (the points closest to the decision boundary) contribute to the margin. A consequence of this is that LR is more sensitive to outliers than SVM. 

For classes that are well separeted SVM tend to perform better than LR, while in more overlapping regimes we usually prefer LR. We also know that logistic regression produces probabilistic values, while SVM produces binary values. This can be an advantage if we want an estimation rather than just the resulting class for each observation.


## d)

FALSE, FALSE, TRUE, TRUE 

## e)

We will know show the connection between hinge loss ans logistic loss for 


# Problem 5

```{r, echo=TRUE, eval=TRUE}
id <- "1VfVCQvWt121UN39NXZ4aR9Dmsbj-p9OU"  # google file ID
GeneData <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id), header = F)
colnames(GeneData)[1:20] = paste(rep("H", 20), c(1:20), sep = "")
colnames(GeneData)[21:40] = paste(rep("D", 20), c(1:20), sep = "")
row.names(GeneData) = paste(rep("G", 1000), c(1:1000), sep = "")
GeneData<-t(GeneData)
```


## a)

We start by performing hierarchical clustering on the dataset. We try the complete, single and average linkage for both the Euclidian and correlation-based distance. We transposed the data in order to cluster the tissues into the two patient groups.

```{r, echo=TRUE, eval=TRUE}
hc.eucl.complete=hclust(dist(GeneData,method="euclidian"), method="complete")
hc.eucl.average=hclust(dist(GeneData,method="euclidian"), method="average")
hc.eucl.single=hclust(dist(GeneData,method="euclidian"), method="single")
```

```{r, echo=TRUE, eval=TRUE}
correlation<-dist(cor(t(GeneData)))
hc.corr.complete=hclust(correlation, method="complete")
hc.corr.average=hclust(correlation, method="average")
hc.corr.single=hclust(correlation, method="single")
```



```{r, echo=TRUE, eval=TRUE}
par(mfrow=c(2,3))
plot(hc.eucl.complete,main="Complete Linkage, Euclidian distance", xlab="", sub="", cex=.9)
plot(hc.eucl.average, main="Average Linkage, Euclidian distance", xlab="", sub="", cex=.9)
plot(hc.eucl.single, main="Single Linkage, Euclidian distance", xlab="", sub="", cex=.9)
plot(hc.corr.complete,main="Complete Linkage, correlation-based distance", xlab="", sub="", cex=.9)
plot(hc.corr.average, main="Average Linkage, correlation-based distance", xlab="", sub="", cex=.9)
plot(hc.corr.single, main="Single Linkage, correlation-based distance", xlab="", sub="", cex=.9)
```

The dendograms seem to recognize that there are two different groups.
## b)

We now use the dendograms to cluster the tissues into two groups.

```{r, echo=TRUE, eval=TRUE}
cutree(hc.eucl.complete, 2)
cutree(hc.eucl.average, 2)
cutree(hc.eucl.single, 2)
```
```{r, echo=TRUE, eval=TRUE}
cutree(hc.corr.complete, 2)
cutree(hc.corr.average, 2)
cutree(hc.corr.single, 2)
```

We know that the first 20 tissues come from healthy patients and the last 20 from diseased ones. Therefore it seems like all linkage and distance measures perform perfectly.


**ASK PEOPLE??? **

## c)


The elements of the vector $\phi$ are called loadings and define a direction in the feature space along which the data varies the most. The data is a $n\times p$ matrix X.


For the first principal component we want $Z_1=\phi_1X$ subject to $||\phi_1||_2=1$. We want $Z_1$ to have the highest possible variance $V(Z_1)=\phi_1^T\Sigma\phi_1$, where $\Sigma$ is the covariance matrix of X.
The first principal component scores are then the column eigenvector corresponding to the largest eigenvalue of $\Sigma$.

## d)

```{r, echo=TRUE, eval=TRUE}
color<-c(rep(1,20),rep(2,20))
```

How to color based on tissues group of patients??? 
  
```{r, echo=TRUE, eval=TRUE}
pca_gene=prcomp(GeneData, scale=TRUE)
plot(pca_gene$x[,1:2], col=c("red","blue")[color],pch=19,xlab="Z1",ylab="Z2")
```

Now we calculate the proportion of variance explained (PVE) by the 5 first components.
```{r, echo=TRUE, eval=TRUE}
pve=100*pca_gene$sdev^2/sum(pca_gene$sdev^2)
cumsum(pve)[5]
```
About 21 percent of the variance is explained by first 5 PCs.

## e)
Use your results from PCA to find which genes that vary the most accross the two groups.
??

```{r, echo=TRUE, eval=TRUE}

nyt_loading = pca_gene$rotation[, 1:2]
informative_loadings = rbind(head(nyt_loading[order(nyt_loading[, 1], decreasing = TRUE), 
                                              ]), head(nyt_loading[order(nyt_loading[, 2], decreasing = TRUE), ]))

biplot(x = pca_gene$x[, 1:2], y = informative_loadings, scale = 0)
```
## f)

```{r, echo=TRUE, eval=TRUE}
km.out = kmeans(GeneData,2, nstart = 20)
km.out$cluster
```

```{r, echo=TRUE, eval=TRUE}

```

```{r, echo=TRUE, eval=TRUE}

```
# References

James, G., D. Witten, T. Hastie, and R. Tibshirani. 2013. An Introduction to Statistical Learning with Applications in R. New York: Springer.




