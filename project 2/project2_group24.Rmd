
---
subtitle: "TMA4268 Statistical Learning V2019"
title: "Compulsory exercise 2: Group 24"
author: "Silje Anfindsen and Clara Panchaud"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
 # html_document
 pdf_document
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=68),tidy=TRUE)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize")
```


# Problem 1

## a)
Let's find the ridge regression estimator. Remember that $\hat{\beta}_{Ridge}$ minimizes $RSS+\lambda\sum_{j=1}^p\beta_j^2$. Let's rewrite this in matrix notation.

$$
\begin{aligned}
min_{\beta}\{(y-X\beta)^T(y-X\beta)+\lambda \beta^T\beta\}&=  &&\text{develop the expression}\\ 
min_{\beta}\{y^Ty-2\beta^TX^Ty+\beta^TX^TX\beta+\lambda \beta^T\beta\}&  &&\text{take the derivative with respect to beta and set equal to 0}\\
-2X^Ty+2X^TX\beta +2\lambda \beta=0 \\
(X^TX+2\lambda I)\beta=X^Ty\\
\beta = (X^TX+\lambda I)^{-1}X^Ty
\end{aligned}
$$
Therefore the estimator is $\hat{\beta}_{Ridge}=(X^TX+\lambda I)^{-1}X^Ty$.

## b)


To find the expected value and the variance-covariance matrix of $\hat{\beta}_{Ridge}$ we need to remember the distribution of y, $y\sim N(X\beta,\sigma^2I)$. Therefore we get the expected value: 

$$E(\hat{\beta}_{Ridge})=E((X^TX+\lambda I)^{-1}X^Ty)=(X^TX+\lambda I)^{-1}X^TE(y)=(X^TX+\lambda I)^{-1}X^TX\beta$$
and the variance-covariance matrix:

$$
\begin{aligned}
Var(\hat{\beta}_{Ridge})=
Var((X^TX+\lambda I)^{-1}X^Ty)&=&&\text{by property of the variance}\\
(X^TX+\lambda I)^{-1}X^TVar(y)((X^TX+\lambda I)^{-1}X^T)^T&= &&\text{develop the expression}\\
\sigma^2(X^TX+\lambda I)^{-1}X^TX(X^TX+\lambda I)^{-1}  \\
\end{aligned}
$$


## c)

TRUE, FALSE, FALSE, TRUE

## d)
```{r, echo=TRUE, eval=TRUE}
library(ISLR)
library(leaps)
library(glmnet)
```

We want to work with the $College$ data. First we split it into a training and a testing set.
```{r, echo=TRUE, eval=TRUE}
set.seed(1)

#make training and testing set 
train.ind = sample(1:nrow(College), 0.5 * nrow(College))
college.train = College[train.ind, ]
college.test = College[-train.ind, ]

#the structure of the data 
str(College)
```

Now we will apply forward selection, using $Outstate$ as a response. We have 18 variables including the response so we will obtain a model including up to 17 variables.
```{r, echo=TRUE, eval=TRUE}
nb_predictors<-17
forward<-regsubsets(Outstate~.,college.train,nvmax=17,method="forward")
sum<-summary(forward)
```

In Figure \ref{fig:fig1} we can look at the RSS and the adjusted $R^2$ in order to pick the number of variables that gives the optimal result. Remember that if the difference is not very significant we would rather pick the simplest model. It seems like 5 variables would be good here.

```{r, echo=TRUE, eval=TRUE, fig1,fig.cap="Comparison of models with different number of variables. \\label{fig:fig1}"}
par(mfrow=c(1,2))
plot(sum$rss,xlab="Number of Variables",ylab="RSS",type="l")
plot(sum$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
```

Below are the chosen variables when we decide to include 5 variables in the reduced model.
```{r, echo=TRUE, eval=TRUE}

nb_selected_pred<-5
variables<-names( coef( forward,id=nb_selected_pred ) )
variables
```

We will now find the reduced model as well as the MSE (mean squared error) on the test set.

```{r, echo=TRUE, eval=TRUE}
#fit the reduced model
reduced.model<-lm(Outstate~Private+Room.Board+Grad.Rate+perc.alumni+Expend, data =college.train)
summary(reduced.model)
```

The reduced model is
$$ \begin{aligned}
\textbf{Outstate} &= `r reduced.model$coefficients[1]` + `r reduced.model$coefficients[2]` \textbf{Private}+`r reduced.model$coefficients[3]` \textbf{Room.Board}\\
&+ `r reduced.model$coefficients[4]` \textbf{Grad.Rate}+ `r reduced.model$coefficients[5]`  \textbf{perc.alumni} +  `r reduced.model$coefficients[6]`  \textbf{Expend}, 
\end{aligned}$$

```{r, echo=TRUE, eval=TRUE}
#find test MSE
p<-predict(reduced.model,newdata=college.test)
error1 <- mean(((college.test$Outstate)-p)^2)
error1
```
The test MSE is
$$MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{f(x_i)})^2 =  `r error1`$$

## e)

We will now select a model for the same dataset as in (d) but this time with the Lasso method. Again, we use both a training and testing set for the data.

```{r, echo=TRUE, eval=TRUE}
#Make a x matrix and y vector for both the training and testing set
x_train<-model.matrix(Outstate~.,college.train)[,-1]
y_train<-college.train$Outstate
x_test<-model.matrix(Outstate~.,college.test)[,-1]
y_test<-college.test$Outstate

```

In order to select the best value for the tuning parameter $\lambda$ we will use cross validation.

```{r, echo=TRUE, eval=TRUE}
set.seed(5555)

#perform the Lasso method and choose the best model using CV
lasso.mod = glmnet(x_train,y_train,alpha=1) #lasso method on train set
cv.lasso = cv.glmnet(x_train,y_train,alpha=1) #CV on train set
lambda.best = cv.lasso$lambda.min #select best lambda
lambda.best

#find the test MSE
predictions<-predict(lasso.mod,s=lambda.best,newx=x_test)
error2 <- mean((predictions-y_test)^2) #test MSE
error2
```

%%Check if log scale!!
From cross validation we can observe that the optimal tuning parameter is $\lambda = `r lambda.best`$ as this is the parameter that minimizes the MSE for the training set. 

The test MSE is now `r error2`, which is lower than what we found for the reduced model using forward selection in d).

The lasso yields sparse models which involves only a subset of variables. Lasso performs variable selection by forcing some of the coefficient estimates to be exactly zero. The selected variables that was not put to zero are displayed below.

```{r, echo=TRUE, eval=TRUE}
c<-coef(lasso.mod,s=lambda.best,exact=TRUE)
inds<-which(c!=0)
variables<-row.names(c)[inds]
variables

```


# Problem 2

## a)

FALSE, FALSE, TRUE, FALSE

## b) 

The basis functions for a cubic spline with knots at each quartile, of variable $X$ are,

$$
\begin{aligned}
&b_0(X) = 1 && b_4(X) = (X-q_1)_{+}^3 \\
&b_1(X) = x && b_5(x) = (X-q_2)_{+}^3 \\
&b_2(X) = x^2 && b_6(X) = (X-q_3)_{+}^3 \\
&b_3(X)=x^3
\end{aligned}
$$

## c)

We will now investigate the realtionship between $Outstate$ and the 6 of the predictors, $Private$, $Room.Board$, $Terminal$, $perc.alumni$, $Expend$, and $Grad.Rate$. 

```{r, echo=TRUE, eval=TRUE}

ds1 = college.train[c("Private", "Outstate")] #binary variable
ds2 = college.train[c("Room.Board", "Outstate")]
ds3 = college.train[c("Terminal", "Outstate")]
ds4 = college.train[c("perc.alumni", "Outstate")]
ds5 = college.train[c("Expend", "Outstate")]
ds6 = college.train[c("Grad.Rate", "Outstate")]

par(mfrow=c(2,3))
plot(ds1)
plot(ds2)
plot(ds3)
plot(ds4)
plot(ds5)
plot(ds6)

```

$Terminal$ and $Expend$ seems to have a non-linear relationship with $Outstate$. The others seems to have a linear realtionship. Check out the binary variable $Private$.

## d)

```{r, echo=TRUE, eval=TRUE}

#make a dataframe
ds = College[c("Terminal", "Outstate")]
n = nrow(ds)

# chosen degrees
deg = 1:10

# plot of training data
plot(ds[train.ind, ], col = "darkgrey", main = "Polynomial regression")
colors = rainbow(length(deg))

# iterate over all degrees (1:4) - could also use a for-loop here
MSE = sapply(deg, function(d) {
    # fit model with this degree
    mod = lm(Outstate ~ poly(Terminal, d), ds[train.ind, ])
    # add lines to the plot 
    lines(cbind(ds[train.ind, 1], mod$fit)[order(ds[train.ind, 1]), ], col = colors[d])
    # calculate mean MSE - this is returned in the MSE variable
    mean((predict(mod, ds[-train.ind, ]) - ds[-train.ind, 2])^2)
})

# add legend to see which color corresponds to which line
par(c(1,1), xpd=TRUE)
legend("topright", inset=c(-0.2,0), legend = paste("d =", deg), lty = 1, col = colors)

```

```{r, echo=TRUE, eval=TRUE}
library(splines)

plot(college.train$Expend, college.train$Outstate, col = "darkgrey", main="Smoothing spline")
fit = smooth.spline(college.train$Expend, college.train$Outstate, cv=TRUE)
fit$df #choose df from CV
lines(fit, col="red", lwd=2)
legend("topright", legend=c("4.6 DF"), col="red", lty=1, lwd=2, cex=.8)
```
We found the degrees of freedom by doing cross validation.

```{r, echo=TRUE, eval=TRUE}
#MSE for polynomial regression models (1-10)
MSE

#MSE for smoothing spline
?predict.smooth.spline
pred = predict(fit, newdata=college.train)
#MSE2 = mean( (college.train$Outstate - pred)^2 )
#MSE2

```


# Problem 3

## a)
False, Not sure ( usually True but not for large trees?), TRUE, FALSE

## b)

```{r, echo=TRUE, eval=TRUE}
library(tree)
tree.mod = tree(Outstate ~ .,college.train)
summary(tree.mod)
```


```{r, echo=TRUE, eval=TRUE}
plot(tree.mod)
text(tree.mod, pretty = 0)
```

```{r, echo=TRUE, eval=TRUE}
set.seed(4268)
cv.college = cv.tree(tree.mod)
tree.min = which.min(cv.college$dev)
best = cv.college$size[tree.min]
plot(cv.college$size, cv.college$dev, type = "b")
points(cv.college$size[tree.min], cv.college$dev[tree.min], col = "red", pch = 20)

```
We see that 7 is on the same level of deviance as for size 10, since this is the smallest number for the size we pick 7. 
Let us now prune the tree to make it size 7.


```{r, echo=TRUE, eval=TRUE}
pr.tree = prune.tree(tree.mod, best = 7)
plot(pr.tree)
text(pr.tree, pretty = 0)

```

## c)

```{r, echo=TRUE, eval=TRUE}


```
# Problem 4

## a)


## b)

## c)


## d)

# Problem 5

## a)

## b)

## c)

## d)

## e)

## f)


# References

James, G., D. Witten, T. Hastie, and R. Tibshirani. 2013. An Introduction to Statistical Learning with Applications in R. New York: Springer.




