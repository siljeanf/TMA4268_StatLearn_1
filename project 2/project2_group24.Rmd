
---
subtitle: "TMA4268 Statistical Learning V2019"
title: "Compulsory exercise 2: Group 24"
author: "Silje Anfindsen and Clara Panchaud"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
 # html_document
 pdf_document
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=68),tidy=TRUE)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize")
```


# Problem 1

## a)
Let's find the ridge regression estimator. Remember that $\hat{\beta}_{Ridge}$ minimizes $RSS+\lambda\sum_{j=1}^p\beta_j^2$. Let's rewrite this in matrix notation.

$$
\begin{aligned}
min_{\beta}\{(y-X\beta)^T(y-X\beta)+\lambda \beta^T\beta\}&=  &&\text{develop the expression}\\ 
min_{\beta}\{y^Ty-2\beta^TX^Ty+\beta^TX^TX\beta+\lambda \beta^T\beta\}&  &&\text{take the derivative with respect to beta and set equal to 0}\\
-2X^Ty+2X^TX\beta +2\lambda \beta=0 \\
(X^TX+2\lambda I)\beta=X^Ty\\
\beta = (X^TX+\lambda I)^{-1}X^Ty
\end{aligned}
$$
Therefore the estimator is $\hat{\beta}_{Ridge}=(X^TX+\lambda I)^{-1}X^Ty$.

## b)


To find the expected value and the variance-covariance matrix of $\hat{\beta}_{Ridge}$ we need to remember the distribution of y, $y\sim N(X\beta,\sigma^2I)$. Therefore we get the expected value: 

$$E(\hat{\beta}_{Ridge})=E((X^TX+\lambda I)^{-1}X^Ty)=(X^TX+\lambda I)^{-1}X^TE(y)=(X^TX+\lambda I)^{-1}X^TX\beta$$
and the variance-covariance matrix:

$$
\begin{aligned}
Var(\hat{\beta}_{Ridge})=
Var((X^TX+\lambda I)^{-1}X^Ty)&=&&\text{by property of the variance}\\
(X^TX+\lambda I)^{-1}X^TVar(y)((X^TX+\lambda I)^{-1}X^T)^T&= &&\text{develop the expression}\\
\sigma^2(X^TX+\lambda I)^{-1}X^TX(X^TX+\lambda I)^{-1}  \\
\end{aligned}
$$


## c)

TRUE, FALSE, FALSE, TRUE

## d)
```{r, echo=TRUE, eval=TRUE}
library(ISLR)
library(leaps)
library(glmnet)
```

We want to work with the $College$ data. First we split it into a training and a testing set.
```{r, echo=TRUE, eval=TRUE}
set.seed(1)

#make training and testing set 
train.ind = sample(1:nrow(College), 0.5 * nrow(College))
college.train = College[train.ind, ]
college.test = College[-train.ind, ]

#the structure of the data 
str(College)
```

Now we will apply forward selection, using $Outstate$ as a response. We have 18 variables including the response so we will obtain a model including up to 17 variables.
```{r, echo=TRUE, eval=TRUE}
nb_predictors<-17
forward<-regsubsets(Outstate~.,college.train,nvmax=17,method="forward")
sum<-summary(forward)
```

In Figure \ref{fig:fig1} we can look at the RSS and the adjusted $R^2$ in order to pick the number of variables that gives the optimal result. Remember that if the difference is not very significant we would rather pick the simplest model. It seems like 5 variables would be good here.

```{r, echo=TRUE, eval=TRUE, fig1,fig.cap="Comparison of models with different number of variables. \\label{fig:fig1}"}
par(mfrow=c(1,2))
plot(sum$rss,xlab="Number of Variables",ylab="RSS",type="l")
plot(sum$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
```

Below are the chosen variables when we decide to include 5 variables in the reduced model.
```{r, echo=TRUE, eval=TRUE}

nb_selected_pred<-5
variables<-names( coef( forward,id=nb_selected_pred ) )
variables
```

We will now find the reduced model as well as the MSE (mean squared error) on the test set.

```{r, echo=TRUE, eval=TRUE}
#fit the reduced model
reduced.model<-lm(Outstate~Private+Room.Board+Grad.Rate+perc.alumni+Expend, data =college.train)
summary(reduced.model)
```

The reduced model is
$$ \begin{aligned}
\textbf{Outstate} &= `r reduced.model$coefficients[1]` + `r reduced.model$coefficients[2]` \textbf{Private}+`r reduced.model$coefficients[3]` \textbf{Room.Board}\\
&+ `r reduced.model$coefficients[4]` \textbf{Grad.Rate}+ `r reduced.model$coefficients[5]`  \textbf{perc.alumni} +  `r reduced.model$coefficients[6]`  \textbf{Expend}, 
\end{aligned}$$

```{r, echo=TRUE, eval=TRUE}
#find test MSE
p<-predict(reduced.model,newdata=college.test)
mse_fwd <- mean(((college.test$Outstate)-p)^2)
mse_fwd
```
The test MSE is
$$MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{f(x_i)})^2 =  `r mse_fwd`$$

## e)

We will now select a model for the same dataset as in (d) but this time with the Lasso method. Again, we use both a training and testing set for the data.

```{r, echo=TRUE, eval=TRUE}
#Make a x matrix and y vector for both the training and testing set
x_train<-model.matrix(Outstate~.,college.train)[,-1]
y_train<-college.train$Outstate
x_test<-model.matrix(Outstate~.,college.test)[,-1]
y_test<-college.test$Outstate

```

In order to select the best value for the tuning parameter $\lambda$ we will use cross validation.

```{r, echo=TRUE, eval=TRUE}
set.seed(5555)

#perform the Lasso method and choose the best model using CV
lasso.mod = glmnet(x_train,y_train,alpha=1) #lasso method on train set
cv.lasso = cv.glmnet(x_train,y_train,alpha=1) #CV on train set
lambda.best = cv.lasso$lambda.min #select best lambda
lambda.best

#find the test MSE
predictions<-predict(lasso.mod,s=lambda.best,newx=x_test)
mse_lasso <- mean((predictions-y_test)^2) #test MSE
mse_lasso
```

%%Check if log scale!!
From cross validation we can observe that the optimal tuning parameter is $\lambda = `r lambda.best`$ as this is the parameter that minimizes the MSE for the training set. 

The test MSE is now `r mse_lasso`, which is lower than what we found for the reduced model using forward selection in d).

The lasso yields sparse models which involves only a subset of variables. Lasso performs variable selection by forcing some of the coefficient estimates to be exactly zero. The selected variables that was not put to zero are displayed below.

```{r, echo=TRUE, eval=TRUE}
c<-coef(lasso.mod,s=lambda.best,exact=TRUE)
inds<-which(c!=0)
variables<-row.names(c)[inds]
variables

```


# Problem 2

## a)

FALSE, FALSE, TRUE, FALSE

## b) 

The basis functions for a cubic spline with knots at each quartile, of variable $X$ are,

$$
\begin{aligned}
&b_0(X) = 1 && b_4(X) = (X-q_1)_{+}^3 \\
&b_1(X) = x && b_5(x) = (X-q_2)_{+}^3 \\
&b_2(X) = x^2 && b_6(X) = (X-q_3)_{+}^3 \\
&b_3(X)=x^3
\end{aligned}
$$

## c)

We will now investigate the realtionship between $Outstate$ and the 6 of the predictors, $Private$, $Room.Board$, $Terminal$, $perc.alumni$, $Expend$, and $Grad.Rate$. 

```{r, echo=TRUE, eval=TRUE}

ds1 = college.train[c("Private", "Outstate")] #binary variable
ds2 = college.train[c("Room.Board", "Outstate")]
ds3 = college.train[c("Terminal", "Outstate")]
ds4 = college.train[c("perc.alumni", "Outstate")]
ds5 = college.train[c("Expend", "Outstate")]
ds6 = college.train[c("Grad.Rate", "Outstate")]

par(mfrow=c(2,3))
plot(ds1)
plot(ds2)
plot(ds3)
plot(ds4)
plot(ds5)
plot(ds6)
```

From each of the plots above we can conclude that at least $Terminal$ and $Expend$ seems to have a non-linear relationship with $Outstate$. These two variables therefore might benefit from a non-linear transformation. The others variables, $Room.Board$, $perc.alumni$ and $Grad.Rate$ seem to have a linear realtionship with the response variable. The binary variable $Private$ is presented through a boxplot. Generally the data seem to cathegorize quite well into these two classes of private and public universites where the trend is a higher out-of-state tuition for private universites, except for some outliers for public universities where the outcome is very high and therefore can seem to belong to a private universities. Anyway, we cannot transform a binary variable. 

## d)

We will now fit several polynomial regression models for $Outstate$ with $Terminal$ as the only covariate. Each polynomial will have a degree from $d=1,..10$. 

```{r, echo=TRUE, eval=TRUE}
library(ggplot2)

#make a dataframe
ds = College[c("Terminal", "Outstate")]
n = nrow(ds)

# chosen degrees
deg = 1:10

#now iterate over each degree d
dat = c()  #make a empty variable to store predicted values for each degree
MSE_poly = c(rep(0,10))  #make a empty variable to store MSE for each degree
for (d in deg) {
    # fit model with this degree
    mod = lm(Outstate ~ poly(Terminal, d), ds[train.ind, ])
    
    #dataframe for Terminal and Outstate showing result for each degree over all samples
    dat = rbind(dat, data.frame(Terminal = ds[train.ind, 1], Outstate = mod$fit, 
                                degree = as.factor(rep(d,length(mod$fit)))))
    # training MSE
    MSE_poly[d] = mean((predict(mod, ds[-train.ind, ]) - ds[-train.ind, 2])^2)
}

# plot fitted values for different degrees
ggplot(data = ds[train.ind, ], aes(x = Terminal, y = Outstate)) + 
    geom_point(color = "darkgrey") + labs(title = "Polynomial regression")+
    geom_line(data = dat, aes(x = Terminal, y = Outstate, color = degree))


```

We will now choice a suitable smoothing spline model to predict $Outstate$ as a function of $Expend$ and plot the fitted function.

```{r, echo=TRUE, eval=TRUE}
library(splines)

#plot training set for Expend as only covariate
plot(college.train$Expend, college.train$Outstate, col = "darkgrey", main="Smoothing spline", xlab="Expend", ylab="Outstate")
#perform CV in order to find optimal number of df
fit = smooth.spline(college.train$Expend, college.train$Outstate, cv=TRUE)
df <- fit$df #choose df from CV
l <- fit$lambda

#add fitted function from smoothing spline
lines(fit, col="red", lwd=2)
legend("topright", legend=c("4.6 DF"), col="red", lty=1, lwd=2, cex=.8)

#training MSE
pred = predict(fit, newdata=college.train)
MSE_spline = mean( (college.train$Outstate - pred$y)^2 )
```

In order to choose a suitable model we did cross validation. The optimal number of degrees of freedom is $df = `r df`$, which gives  the smoothing parameter $\lambda = `r l`$

We will now calculate the training MSE for the polynomial regresson models and the smoothing spline model.
For the polynomial regression models we find it easiest to look at the MSE by presenting a plot with MSE for each degree.

```{r, echo=TRUE, eval=TRUE}
#MSE for polynomial regression models (1-10) 
plot(1:10,MSE_poly, type = "o", pch = 16, xlab = "degree", main = "Test error")
min(MSE_poly)

#MSE for smoothing spline
MSE_spline

```
The training MSE for the two methods are (discuss is it is as expected!)

# Problem 3

## a)
FALSE, TRUE, TRUE, FALSE

## b)

To predict $Outstate$ we first try fitting the simple tree-based method, Regression Trees. Since we have learned that decision trees often suffer from high variance we will divide our training data into two new sets and fit a tree on each. If this is the case we will get two quite different trees. 

```{r, echo=TRUE, eval=TRUE}
library(tree)
set.seed(1)

#make new training and testing set - random split
new.train.ind = sample(1:nrow(college.train), 0.5 * nrow(college.train))
new.college.train1 = college.train[new.train.ind, ]
new.college.train2 = college.train[-new.train.ind, ]

#fit a tree on the full data set and two new trees with the two new traning sets
full.tree = tree(Outstate ~ .,college.train) #full tree
tree.mod1 = tree(Outstate ~ .,new.college.train1)
tree.mod2 = tree(Outstate ~ .,new.college.train2)

#plot the two tree based on a smaller part of the training set
par(mfrow=c(1,2))
plot(tree.mod1)
text(tree.mod1, pretty = 0)
plot(tree.mod2)
text(tree.mod2, pretty = 0)

#find test MSE for full tree
yhat = predict(full.tree, newdata = college.test, n.trees = 500)
mse_pure <- mean((yhat- college.test$Outstate)^2)
mse_pure

```
### maybe we dont need to actually do the pruning, and go directly  boosting? 
Now we try pruning and check whether this will improve performance.

```{r, echo=TRUE, eval=TRUE}

set.seed(1)
cv.college = cv.tree(full.tree)
tree.min = which.min(cv.college$dev)
best = cv.college$size[tree.min]
plot(cv.college$size, cv.college$dev, type = "b")
points(cv.college$size[tree.min], cv.college$dev[tree.min], col = "red", pch = 20)

```

We see that 7 is on the same level of deviance as for size 10, since this is the smallest number for the size we pick 7. Let us now prune the tree to make it size 7.
```{r, echo=TRUE, eval=TRUE}
pr.tree = prune.tree(tree.mod1, best = 7)
#plot(pr.tree)
#text(pr.tree, pretty = 0)

#find test mse
yhat.prune = predict(pr.tree, newdata = college.test, n.trees = 500)
mse.prune <- mean((yhat.prune- college.test$Outstate)^2)
mse.prune

```
### done with pruning (maybe remove the pruning part ?)

We observe that the two decision trees above are quite different where different variables act as the most important factors for splitting. We conclude that the regression tree therefore suffer from high variance. We will try the boosting approach in order to train the tree and decrease the variance. The boosting approach grows several trees where each tree is grown using information from previously grown trees. The final predicitor is a weighted sum of the trees. We have three tuning parameters: the number of trees $B$, the shrinkage parameter $\lambda$ and the number of splits in each tree (interaction depth) $d$. These parameters can be decided with cross validation.

```{r, echo=TRUE, eval=TRUE}
library(gbm)
set.seed(1)

#fit a boosted tree to the training data - use CV in order to decide the tuning parameters
boost.cv = gbm(Outstate ~ .,data=college.train, distribution="gaussian", cv.folds = 10)
boost.cv$n.trees

#check relative importance of variables 
summary(boost.cv, plotit = FALSE)

```

We notice that $Expend$ has a very high relative influence and therefore seems to be the most important variable in the data.
We will now look at the test MSE for the method. 
```{r, echo=TRUE, eval=TRUE}

yhat.boost = predict(boost.cv, newdata = college.test)
mse_boost <- mean((yhat.boost - college.test$Outstate)^2)
mse_boost

```

The test MSE for the boosted tree, $MSE_{boosted} = `r mse_boost`$ is smaller than for the Regression tree approach, $MSE_{non-boosted} = `r mse_pure`$. Therefore the boosted tree will probably do a better prediciton of the response. **Write something about pros/cons for the chosen method (boosting) maybe compared to pure regression trees.**

## c)

We will now compare the test MSEs among the methods used on the data set $College$ so far. That is: the two linear model selection methods, forward selection and Lasso method, non-linear methods, polynomial regression and smoothing splines and at last the tree-based method Boosted regression trees.
```{r, echo=TRUE, eval=TRUE}

MSE <- c(mse_fwd,mse_lasso,min(MSE_poly),MSE_spline,mse_pure, mse_boost)
Method <- c("Forward selection", "Lasso", "Polynomial regression", "Smoothing spline", "Regression Tree", "Boosting")
df <- data.frame(Method, MSE)
kable(df)
```


The method performing best in terms of prediction error is the boosted regression tree. But if the aim is to develop a interpretable model we would probably have chosen the regression tree.


# Problem 4

Start by loading the data of $diabetes$ from a population of women.

```{r, echo=TRUE, eval=TRUE}
id <- "1Fv6xwKLSZHldRAC1MrcK2mzdOYnbgv0E"  # google file ID
d.diabetes <- dget(sprintf("https://docs.google.com/uc?id=%s&export=download", 
    id))
d.train = d.diabetes$ctrain
d.test = d.diabetes$ctest
```

## a)
In order to anseer on the Mulitple choice we have to present the training data.

```{r, echo=TRUE, eval=TRUE}
summary(d.train)
max(d.train$npreg)#max nr of pregnancies
head(d.train) #overview of data
plot(d.train) #look at correlation between variables
plot(diabetes~npreg, data=d.train)

```

```{r, echo=TRUE, eval=TRUE}
ggplot(d.train,aes(x=glu,y=bmi,color=diabetes))+geom_point()
```


TRUE, TRUE, TRUE, FALSE (not sure about the first and last)

## b)

We will now fit a support vector classifier with linear boundary and a support vector machine with radial boundary to find good functions that predict the diabetes status of a patient. 

```{r, echo=TRUE, eval=TRUE}

#make response variable a factor
d.train$diabetes <- as.factor(d.train$diabetes)
d.test$diabetes <- as.factor(d.test$diabetes)

set.seed(10111)

#make a grid


#
```

## c)




## d)

```{r, echo=TRUE, eval=TRUE}

```
# Problem 5

```{r, echo=TRUE, eval=TRUE}
id <- "1VfVCQvWt121UN39NXZ4aR9Dmsbj-p9OU"  # google file ID
GeneData <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", 
    id), header = F)
colnames(GeneData)[1:20] = paste(rep("H", 20), c(1:20), sep = "")
colnames(GeneData)[21:40] = paste(rep("D", 20), c(1:20), sep = "")
row.names(GeneData) = paste(rep("G", 1000), c(1:1000), sep = "")
```

## a)


We start by performing hierarchical clustering on the dataset. We try the complete, single and average linkage for both the Euclidian and correlation-based distance.

```{r, echo=TRUE, eval=TRUE}
hc.eucl.complete=hclust(dist(t(GeneData),method="euclidian"), method="complete")
hc.eucl.average=hclust(dist(t(GeneData),method="euclidian"), method="average")
hc.eucl.single=hclust(dist(t(GeneData),method="euclidian"), method="single")
```

```{r, echo=TRUE, eval=TRUE}
correlation<-dist(cor(GeneData))
hc.corr.complete=hclust(correlation, method="complete")
hc.corr.average=hclust(correlation, method="average")
hc.corr.single=hclust(correlation, method="single")
```



```{r, echo=TRUE, eval=TRUE}
par(mfrow=c(2,3))
plot(hc.eucl.complete,main="Complete Linkage, Euclidian distance", xlab="", sub="", cex=.9)
plot(hc.eucl.average, main="Average Linkage, Euclidian distance", xlab="", sub="", cex=.9)
plot(hc.eucl.single, main="Single Linkage, Euclidian distance", xlab="", sub="", cex=.9)
plot(hc.corr.complete,main="Complete Linkage, correlation-based distance", xlab="", sub="", cex=.9)
plot(hc.corr.average, main="Average Linkage, correlation-based distance", xlab="", sub="", cex=.9)
plot(hc.corr.single, main="Single Linkage, correlation-based distance", xlab="", sub="", cex=.9)

```

The dendograms seem to recognize that there are two different groups.
## b)


We now use the dendograms to cluster the tissues into two groups.

```{r, echo=TRUE, eval=TRUE}
cutree(hc.eucl.complete, 2)
cutree(hc.eucl.average, 2)
cutree(hc.eucl.single, 2)
```
```{r, echo=TRUE, eval=TRUE}
cutree(hc.corr.complete, 2)
cutree(hc.corr.average, 2)
cutree(hc.corr.single, 2)
```

We know that the first 20 come from one group and the rest from the other group. Therefore it seems like all linkage and distance measure perform perfectly.

## c)



The elements of the vector $\phi$ are called loadings and define a direction in the feature space along which the data varies the most. The data is a $n\times p$ matrix X.


For the first principal component we want $Z_1=\phi_1X$ subject to $||\phi_1||_2=1$. We want $Z_1$ to have the highest possible variance $V(Z_1)=\phi_1^T\Sigma\phi_1$, where $\Sigma$ is the covariance matrix of X.
The first principal component scores are then the column eigenvector corresponding to the largest eigenvalue of $\Sigma$.
## d)



```{r, echo=TRUE, eval=TRUE}
color<-c(rep(1,200),rep(2,300))
```

How to color based on tissues group of patients??? 

```{r, echo=TRUE, eval=TRUE}
pca_gene=prcomp(GeneData, scale=TRUE)
plot(pca_gene$x[,1:2], col=c("red","blue")[color],pch=19,xlab="Z1",ylab="Z2")
```

Now we calculate the proportion of variance explained (PVE) by the 5 first components.
```{r, echo=TRUE, eval=TRUE}
pve=100*pca_gene$sdev^2/sum(pca_gene$sdev^2)
cumsum(pve)[5]
```
About 28 percent of the variance is explained by first 5 PC (?)

```{r, echo=TRUE, eval=TRUE}

```

## e)
Use your results from PCA to find which genes that vary the most accross the two groups.
??
```{r, echo=TRUE, eval=TRUE}

nyt_loading = pca_gene$rotation[, 1:2]
informative_loadings = rbind(head(nyt_loading[order(nyt_loading[, 1], decreasing = TRUE), 
    ]), head(nyt_loading[order(nyt_loading[, 2], decreasing = TRUE), ]))

biplot(x = pca_gene$x[, 1:2], y = informative_loadings, scale = 0)
```
## f)

```{r, echo=TRUE, eval=TRUE}
km.out = kmeans(t(GeneData), 2, nstart = 20)
km.out$cluster
```


# References

James, G., D. Witten, T. Hastie, and R. Tibshirani. 2013. An Introduction to Statistical Learning with Applications in R. New York: Springer.




