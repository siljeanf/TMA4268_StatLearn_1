
---
subtitle: "TMA4268 Statistical Learning V2019"
title: "Compulsory exercise 2: Group 24"
author: "Silje Anfindsen and Clara Panchaud"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
 # html_document
 pdf_document
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=68),tidy=TRUE)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize")
```


# Problem 1

## a)
Let's find the ridge regression estimator. Remember that $\hat{\beta}_{Ridge}$ minimizes $RSS+\lambda\sum_{j=1}^p\beta_j^2$. Let's rewrite this in matrix notation.

$$
\begin{aligned}
min_{\beta}\{(y-X\beta)^T(y-X\beta)+\lambda \beta^T\beta\}&=  &&\text{develop the expression}\\ 
min_{\beta}\{y^Ty-2\beta^TX^Ty+\beta^TX^TX\beta+\lambda \beta^T\beta\}&  &&\text{take the derivative with respect to beta and set equal to 0}\\
-2X^Ty+2X^TX\beta +2\lambda \beta=0 \\
(X^TX+2\lambda I)\beta=X^Ty\\
\beta = (X^TX+\lambda I)^{-1}X^Ty
\end{aligned}
$$
Therefore the estimator is $\hat{\beta}_{Ridge}=(X^TX+\lambda I)^{-1}X^Ty$.

## b)


To find the expected value and the variance-covariance matrix of $\hat{\beta}_{Ridge}$ we need to remember the distribution of y, $y\sim N(X\beta,\sigma^2I)$. Therefore we get the expected value: 

$$E(\hat{\beta}_{Ridge})=E((X^TX+\lambda I)^{-1}X^Ty)=(X^TX+\lambda I)^{-1}X^TE(y)=(X^TX+\lambda I)^{-1}X^TX\beta$$
and the variance-covariance matrix:

$$
\begin{aligned}
Var(\hat{\beta}_{Ridge})=
Var((X^TX+\lambda I)^{-1}X^Ty)&=&&\text{by property of the variance}\\
(X^TX+\lambda I)^{-1}X^TVar(y)((X^TX+\lambda I)^{-1}X^T)^T&= &&\text{develop the expression}\\
\sigma^2(X^TX+\lambda I)^{-1}X^TX(X^TX+\lambda I)^{-1}  \\
\end{aligned}
$$


## c)

TRUE, FALSE, FALSE, TRUE

## d)
```{r, echo=TRUE, eval=TRUE}
library(ISLR)
library(leaps)
library(glmnet)
```

We want to work with the $College$ data. First we split it into a training and a testing set.
```{r, echo=TRUE, eval=TRUE}
set.seed(1)

#make training and testing set 
train.ind = sample(1:nrow(College), 0.5 * nrow(College))
college.train = College[train.ind, ]
college.test = College[-train.ind, ]

#the structure of the data 
str(College)
```

Now we will apply forward selection, using $Outstate$ as a response. We have 18 variables including the response so we will obtain a model including up to 17 variables.
```{r, echo=TRUE, eval=TRUE}
nb_predictors<-17
forward<-regsubsets(Outstate~.,college.train,nvmax=17,method="forward")
sum<-summary(forward)
```

In Figure \ref{fig:fig1} we can look at the RSS and the adjusted $R^2$ in order to pick the number of variables that gives the optimal result. Remember that if the difference is not very significant we would rather pick the simplest model. It seems like 5 variables would be good here.

```{r, echo=TRUE, eval=TRUE, fig1,fig.cap="Comparison of models with different number of variables. \\label{fig:fig1}"}
par(mfrow=c(1,2))
plot(sum$rss,xlab="Number of Variables",ylab="RSS",type="l")
plot(sum$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
```

Below are the chosen variables when we decide to include 5 variables in the reduced model.
```{r, echo=TRUE, eval=TRUE}

nb_selected_pred<-5
variables<-names( coef( forward,id=nb_selected_pred ) )
variables
```

We will now find the reduced model as well as the MSE (mean squared error) on the test set.

```{r, echo=TRUE, eval=TRUE}
#fit the reduced model
reduced.model<-lm(Outstate~Private+Room.Board+Grad.Rate+perc.alumni+Expend, data =college.train)
summary(reduced.model)
```

The reduced model is
$$ \begin{aligned}
\textbf{Outstate} &= `r reduced.model$coefficients[1]` + `r reduced.model$coefficients[2]` \textbf{Private}+`r reduced.model$coefficients[3]` \textbf{Room.Board}\\
&+ `r reduced.model$coefficients[4]` \textbf{Grad.Rate}+ `r reduced.model$coefficients[5]`  \textbf{perc.alumni} +  `r reduced.model$coefficients[6]`  \textbf{Expend}, 
\end{aligned}$$

```{r, echo=TRUE, eval=TRUE}
#find test MSE
p<-predict(reduced.model,newdata=college.test)
error1 <- mean(((college.test$Outstate)-p)^2)
error1
```
The test MSE is
$$MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{f(x_i)})^2 =  `r error1`$$

## e)

We will now select a model for the same dataset as in (d) but this time with the Lasso method. Again, we use both a training and testing set for the data.

```{r, echo=TRUE, eval=TRUE}
#Make a x matrix and y vector for both the training and testing set
x_train<-model.matrix(Outstate~.,college.train)[,-1]
y_train<-college.train$Outstate
x_test<-model.matrix(Outstate~.,college.test)[,-1]
y_test<-college.test$Outstate

```

In order to select the best value for the tuning parameter $\lambda$ we will use cross validation.

```{r, echo=TRUE, eval=TRUE}
set.seed(5555)

#perform the Lasso method and choose the best model using CV
lasso.mod = glmnet(x_train,y_train,alpha=1) #lasso method on train set
cv.lasso = cv.glmnet(x_train,y_train,alpha=1) #CV on train set
lambda.best = cv.lasso$lambda.min #select best lambda
lambda.best

#find the test MSE
predictions<-predict(lasso.mod,s=lambda.best,newx=x_test)
error2 <- mean((predictions-y_test)^2) #test MSE
error2
```

%%Check if log scale!!
From cross validation we can observe that the optimal tuning parameter is $\lambda = `r lambda.best`$ as this is the parameter that minimizes the MSE for the training set. 

The test MSE is now `r error2`, which is lower than what we found for the reduced model using forward selection in d).

The lasso yields sparse models which involves only a subset of variables. Lasso performs variable selection by forcing some of the coefficient estimates to be exactly zero. The selected variables that was not put to zero are displayed below.

```{r, echo=TRUE, eval=TRUE}
c<-coef(lasso.mod,s=lambda.best,exact=TRUE)
inds<-which(c!=0)
variables<-row.names(c)[inds]
variables

```


# Problem 2

## a)

FALSE, FALSE, TRUE, FALSE

## b) 

The basis functions for a cubic spline with knots at each quartile, of variable $X$ are,

$$
\begin{aligned}
&b_0(X) = 1 && b_4(X) = (X-q_1)_{+}^3 \\
&b_1(X) = x && b_5(x) = (X-q_2)_{+}^3 \\
&b_2(X) = x^2 && b_6(X) = (X-q_3)_{+}^3 \\
&b_3(X)=x^3
\end{aligned}
$$

## c)

We will now investigate the realtionship between $Outstate$ and the 6 of the predictors, $Private$, $Room.Board$, $Terminal$, $perc.alumni$, $Expend$, and $Grad.Rate$. 

```{r, echo=TRUE, eval=TRUE}

ds1 = college.train[c("Private", "Outstate")] #binary variable
ds1
ds2 = college.train[c("Room.Board", "Outstate")]
ds3 = college.train[c("Terminal", "Outstate")]
ds4 = college.train[c("perc.alumni", "Outstate")]
ds5 = college.train[c("Expend", "Outstate")]
ds6 = college.train[c("Grad.Rate", "Outstate")]

par(mfrow=c(2,3))
plot(ds1)
plot(ds2)
plot(ds3)
plot(ds4)
plot(ds5)
plot(ds6)
```

From each of the plots above we can conclude that at least $Terminal$ and $Expend$ seems to have a non-linear relationship with $Outstate$. These two variables therefore might benefit from a non-linear transformation. The others variables, $Room.Board$, $perc.alumni$ and $Grad.Rate$ seem to have a linear realtionship with the response variable. The binary variable $Private$ is presented through a boxplot. Generally the data seem to cathegorize quite well into these two classes of private and public universites where the trend is a higher Outlier for private universites, except for some outliers for public universities where the outcome is very high and therefore can seem to belong to a private universities. Anyway, we cannot transform a binary variable. 

## d)

We will now fit several polynomial regression models for $Outstate$ with $Terminal$ as the only covariate. Each polynomial will have a degree from $d=1,..10$. 

```{r, echo=TRUE, eval=TRUE}
library(ggplot2)

#make a dataframe
ds = College[c("Terminal", "Outstate")]
n = nrow(ds)

# chosen degrees
deg = 1:10

#now iterate over each degree d
dat = c()  #make a empty variable to store predicted values for each degree
MSE = list(rep(0, 10))  #make a empty variable to store MSE for each degree

for (d in deg) {
    # fit model with this degree
    mod = lm(Outstate ~ poly(Terminal, d), ds[train.ind, ])
    
    #dataframe for Terminal and Outstate showing result for each degree over all samples
    dat = rbind(dat, data.frame(Terminal = ds[train.ind, 1], Outstate = mod$fit, 
                                degree = as.factor(rep(d,length(mod$fit)))))
    # training MSE
    MSE[d] = mean((predict(mod, ds[-train.ind, ]) - ds[-train.ind, 2])^2)
}

# plot fitted values for different degrees
ggplot(data = ds[train.ind, ], aes(x = Terminal, y = Outstate)) + 
    geom_point(color = "darkgrey") + labs(title = "Polynomial regression")+
    geom_line(data = dat, aes(x = Terminal, y = Outstate, color = degree))


```

We will now choice a suitable smoothing spline model to predict $Outstate$ as a function of $Expend$ and plot the fitted function.

```{r, echo=TRUE, eval=TRUE}
library(splines)

#plot training set for Expend as only covariate
plot(college.train$Expend, college.train$Outstate, col = "darkgrey", main="Smoothing spline", xlab="Expend", ylab="Outstate")
#perform CV in order to find optimal number of df
fit = smooth.spline(college.train$Expend, college.train$Outstate, cv=TRUE)
fit$df #choose df from CV
fit$lambda

#add fitted function from smoothing spline
lines(fit, col="red", lwd=2)
legend("topright", legend=c("4.6 DF"), col="red", lty=1, lwd=2, cex=.8)

#training MSE
pred = predict(fit, newdata=college.train)
MSE2 = mean( (college.train$Outstate - pred$y)^2 )
```

In order to choose a suitable model we did cross validation. The optimal number of degrees of freedom is $df = 4.6$, which gives  the smoothing parameter $\lambda = 0.0075$

We will now calculate the training MSE for the polynomial regresson models and the smoothing spline model.
For the polynomial regression models we find it easiest to look at the MSE by presenting a plot with MSE for each degree.

```{r, echo=TRUE, eval=TRUE}
#MSE for polynomial regression models (1-10) 
plot(1:10,MSE, type = "o", pch = 16, xlab = "degree", main = "Test error")
MSE[3]

#MSE for smoothing spline
MSE2

```
The training MSE for the two methods are (discuss is it is as expected!)

# Problem 3

## a)
FALSE, TRUE, TRUE, FALSE

## b)

We will now select the Regression Trees-method in order to predict $Outstate$. We fit two regression trees by dividing our training data into two new sets in order to investigate if we have high variance in our data and should use minimze it. 

```{r, echo=TRUE, eval=TRUE}
library(tree)

#make new training and testing set 
new.train.ind = sample(1:nrow(college.train), 0.5 * nrow(college.train))
new.college.train1 = college.train[new.train.ind, ]
new.college.train2 = college.train[-new.train.ind, ]

tree.mod1 = tree(Outstate ~ .,new.college.train1)
tree.mod2 = tree(Outstate ~ .,new.college.train2)

```


```{r, echo=TRUE, eval=TRUE}
plot(tree.mod1)
text(tree.mod1, pretty = 0)

plot(tree.mod2)
text(tree.mod2, pretty = 0)
```

```{r, echo=TRUE, eval=TRUE}
set.seed(4268)
cv.college = cv.tree(tree.mod)
tree.min = which.min(cv.college$dev)
best = cv.college$size[tree.min]
plot(cv.college$size, cv.college$dev, type = "b")
points(cv.college$size[tree.min], cv.college$dev[tree.min], col = "red", pch = 20)

```

We see that 7 is on the same level of deviance as for size 10, since this is the smallest number for the size we pick 7. 
Let us now prune the tree to make it size 7.


```{r, echo=TRUE, eval=TRUE}
pr.tree = prune.tree(tree.mod, best = 7)
plot(pr.tree)
text(pr.tree, pretty = 0)

```

## c)

```{r, echo=TRUE, eval=TRUE}


```
# Problem 4

## a)


## b)

## c)


## d)

# Problem 5

## a)

## b)

## c)

## d)

## e)

## f)


# References

James, G., D. Witten, T. Hastie, and R. Tibshirani. 2013. An Introduction to Statistical Learning with Applications in R. New York: Springer.




