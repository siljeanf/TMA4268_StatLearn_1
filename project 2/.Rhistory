MSE = list(rep(0, 10))  #make a empty variable to store MSE for each degree
for (d in deg) {
# fit model with this degree
mod = lm(Outstate ~ poly(Terminal, d), ds[train.ind, ])
#dataframe for Terminal and Outstate showing result for each degree over all samples
dat = rbind(dat, data.frame(Terminal = ds[train.ind, 1], Outstate = mod$fit,
degree = as.factor(rep(d,length(mod$fit)))))
# training MSE
MSE[d] = mean((predict(mod, ds[-train.ind, ]) - ds[-train.ind, 2])^2)
}
# plot fitted values for different degrees
ggplot(data = ds[train.ind, ], aes(x = Terminal, y = Outstate)) +
geom_point(color = "darkgrey") + labs(title = "Polynomial regression")+
geom_line(data = dat, aes(x = Terminal, y = Outstate, color = degree))
library(splines)
#plot training set for Expend as only covariate
plot(college.train$Expend, college.train$Outstate, col = "darkgrey", main="Smoothing spline", xlab="Expend", ylab="Outstate")
#perform CV in order to find optimal number of df
fit = smooth.spline(college.train$Expend, college.train$Outstate, cv=TRUE)
fit$df #choose df from CV
#add fitted function from smoothing spline
lines(fit, col="red", lwd=2)
legend("topright", legend=c("4.6 DF"), col="red", lty=1, lwd=2, cex=.8)
#MSE for polynomial regression models (1-10)
#make dataframe for MSE for each degree
MSEdata = data.frame(MSE = MSE, degree = 1:10)
ggplot(data = MSEdata, aes(x = degree, y = MSE)) + geom_line() + geom_point() +
labs(title = "Test error")
#MSE for polynomial regression models (1-10)
MSEdata = data.frame(MSE = MSE, degree = 1:10)
ggplot(data = MSEdata, aes(x = degree, y = MSE)) + geom_line() + geom_point() +
labs(title = "Test error")
MSEdata
#MSE for polynomial regression models (1-10)
MSE
#MSE for smoothing spline
pred = predict(fit, newdata=college.train)
#MSE2 = mean( (college.train$Outstate - pred)^2 )
#MSE2
#MSE for polynomial regression models (1-10)
MSE
#MSE for polynomial regression models (1-10)
plot(MSE)
MSEdata
#MSE for polynomial regression models (1-10)
# plot MSE
MSEdata = data.frame(MSE = MSE, degree = 1:10)
ggplot(data = MSEdata, aes(x = degree, y = MSEdata)) + geom_line() + geom_point() +
labs(title = "Test error")
#MSE for polynomial regression models (1-10)
# plot MSE
MSEdata = data.frame(MSE = MSE, degree = 1:10)
ggplot(data = MSEdata, aes(x = degree, y = MSE)) + geom_line() + geom_point() +
labs(title = "Test error")
ggplot(data = MSEdata, aes(x = degree, y = MSEdata)) + geom_line() + geom_point() +
labs(title = "Test error")
#MSE for polynomial regression models (1-10)
# plot MSE
MSEdata = data.frame(MSE = MSE, degree = 1:10)
length(MSEdata)
ggplot(data = MSEdata, aes(x = degree, y = MSEdata)) + geom_line() + geom_point() +
labs(title = "Test error")
MSEdata
length(MSE)
MSE
ggplot(data = MSEdata, aes(x = degree, y = MSE)) + geom_line() + geom_point() +
labs(title = "Test error")
MSE
#MSE for polynomial regression models (1-10)
# plot MSE
plot(1:10,MSE)
#MSE for smoothing spline
pred = predict(fit, newdata=college.train)
#MSE2 = mean( (college.train$Outstate - pred)^2 )
#MSE2
#MSE for polynomial regression models (1-10)
# plot MSE
plot(1:10,MSE, type = "o", pch = 16, xlab = "degree", main = "Test error")
#MSE for smoothing spline
pred = predict(fit, newdata=college.train)
#MSE2 = mean( (college.train$Outstate - pred)^2 )
#MSE2
pred = predict(fit, newdata=college.train)
pred
pred = predict(fit, newdata=college.train)
MSE2 = mean( (college.train$Outstate - pred$y)^2 )
MSE2
#MSE for polynomial regression models (1-10)
plot(1:10,MSE, type = "o", pch = 16, xlab = "degree", main = "Test error")
#MSE for smoothing spline
MSE2
MSE[9]
max(MSE)
max(MSE)
#MSE for polynomial regression models (1-10)
plot(1:10,MSE, type = "o", pch = 16, xlab = "degree", main = "Test error")
#MSE for smoothing spline
MSE2
#MSE for polynomial regression models (1-10)
plot(1:10,MSE, type = "o", pch = 16, xlab = "degree", main = "Test error")
MSE
#MSE for smoothing spline
MSE2
#MSE for polynomial regression models (1-10)
plot(1:10,MSE, type = "o", pch = 16, xlab = "degree", main = "Test error")
MSE[9]
#MSE for smoothing spline
MSE2
fit$lambda
fit$lambda
#MSE for polynomial regression models (1-10)
plot(1:10,MSE, type = "o", pch = 16, xlab = "degree", main = "Test error")
MSE[9]
#MSE for smoothing spline
MSE2
MSE[3]
#MSE for smoothing spline
MSE2
MSE[3]
#MSE for smoothing spline
MSE2
pr.tree = prune.tree(tree.mod, best = 7)
library(tree)
tree.mod = tree(Outstate ~ .,college.train)
summary(tree.mod)
plot(tree.mod)
text(tree.mod, pretty = 0)
train.ind
library(tree)
#make new training and testing set
new.train.ind = sample(1:nrow(college.train), 0.5 * nrow(college.train))
new.college.train1 = College[new.train.ind, ]
new.college.train2 = College[-new.train.ind, ]
tree.mod = tree(Outstate ~ .,new.college.train)
library(tree)
#make new training and testing set
new.train.ind = sample(1:nrow(college.train), 0.5 * nrow(college.train))
new.college.train1 = college.train[new.train.ind, ]
new.college.train2 = college.train[-new.train.ind, ]
tree.mod = tree(Outstate ~ .,new.college.train)
library(tree)
#make new training and testing set
new.train.ind = sample(1:nrow(college.train), 0.5 * nrow(college.train))
new.college.train1 = college.train[new.train.ind, ]
new.college.train2 = college.train[-new.train.ind, ]
tree.mod = tree(Outstate ~ .,new.college.train1)
summary(tree.mod)
library(tree)
#make new training and testing set
new.train.ind = sample(1:nrow(college.train), 0.5 * nrow(college.train))
new.college.train1 = college.train[new.train.ind, ]
new.college.train2 = college.train[-new.train.ind, ]
tree.mod1 = tree(Outstate ~ .,new.college.train1)
tree.mod2 = tree(Outstate ~ .,new.college.train2)
summary(tree.mod)
plot(tree.mod1)
text(tree.mod1, pretty = 0)
plot(tree.mod2)
text(tree.mod2, pretty = 0)
install.packages("formattable")
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=68),tidy=TRUE)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
cache=TRUE, size="scriptsize")
id <- "1VfVCQvWt121UN39NXZ4aR9Dmsbj-p9OU"  # google file ID
GeneData <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download",
id), header = F)
colnames(GeneData)[1:20] = paste(rep("H", 20), c(1:20), sep = "")
colnames(GeneData)[21:40] = paste(rep("D", 20), c(1:20), sep = "")
row.names(GeneData) = paste(rep("G", 1000), c(1:1000), sep = "")
head(GeneData)
hc.complete=hclust(dist(GeneData), method="complete")
hc.average=hclust(dist(GeneData), method="average")
hc.single=hclust(dist(GeneData), method="single")
par(mfrow=c(1,3))
plot(hc.complete,main="Complete Linkage", xlab="", sub="", cex=.9)
plot(hc.average, main="Average Linkage", xlab="", sub="", cex=.9)
plot(hc.single, main="Single Linkage", xlab="", sub="", cex=.9)
id <- "1VfVCQvWt121UN39NXZ4aR9Dmsbj-p9OU"  # google file ID
GeneData <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download",
id), header = F)
colnames(GeneData)[1:20] = paste(rep("H", 20), c(1:20), sep = "")
colnames(GeneData)[21:40] = paste(rep("D", 20), c(1:20), sep = "")
row.names(GeneData) = paste(rep("G", 1000), c(1:1000), sep = "")
GeneData=t(GeneData)
hc.complete=hclust(dist(GeneData), method="complete")
hc.average=hclust(dist(GeneData), method="average")
hc.single=hclust(dist(GeneData), method="single")
par(mfrow=c(1,3))
plot(hc.complete,main="Complete Linkage", xlab="", sub="", cex=.9)
plot(hc.average, main="Average Linkage", xlab="", sub="", cex=.9)
plot(hc.single, main="Single Linkage", xlab="", sub="", cex=.9)
hc.complete=hclust(dist(GeneData,method="euclidian"), method="complete")
hc.average=hclust(dist(GeneData), method="average")
hc.single=hclust(dist(GeneData), method="single")
par(mfrow=c(1,3))
plot(hc.complete,main="Complete Linkage", xlab="", sub="", cex=.9)
plot(hc.average, main="Average Linkage", xlab="", sub="", cex=.9)
plot(hc.single, main="Single Linkage", xlab="", sub="", cex=.9)
par(mfrow=c(2,3))
plot(hc.eucl.complete,main="Complete Linkage, Euclidian distance", xlab="", sub="", cex=.9)
hc.eucl.complete=hclust(dist(GeneData,method="euclidian"), method="complete")
hc.eucl.average=hclust(dist(GeneData,method="euclidian"), method="average")
hc.eucl.single=hclust(dist(GeneData,method="euclidian"), method="single")
correlation<-as.dist(1-cor(t(GeneData)))
hc.corr.complete=hclust(dd, method="complete")
correlation<-as.dist(1-cor(t(GeneData)))
hc.corr.complete=hclust(correlation, method="complete")
hc.corr.average=hclust(correlation, method="average")
hc.corr.single=hclust(correlation, method="single")
par(mfrow=c(2,3))
plot(hc.eucl.complete,main="Complete Linkage, Euclidian distance", xlab="", sub="", cex=.9)
plot(hc.eucl.average, main="Average Linkage, Euclidian distance", xlab="", sub="", cex=.9)
plot(hc.eucl.single, main="Single Linkage, Euclidian distance", xlab="", sub="", cex=.9)
plot(hc.corr.complete,main="Complete Linkage, correlation-based distance", xlab="", sub="", cex=.9)
plot(hc.corr.average, main="Average Linkage, correlation-based distance", xlab="", sub="", cex=.9)
plot(hc.corr.single, main="Single Linkage, correlation-based distance", xlab="", sub="", cex=.9)
correlation<-as.dist(1-cor(GeneData))
hc.corr.complete=hclust(correlation, method="complete")
hc.corr.average=hclust(correlation, method="average")
hc.corr.single=hclust(correlation, method="single")
par(mfrow=c(2,3))
plot(hc.eucl.complete,main="Complete Linkage, Euclidian distance", xlab="", sub="", cex=.9)
plot(hc.eucl.average, main="Average Linkage, Euclidian distance", xlab="", sub="", cex=.9)
plot(hc.eucl.single, main="Single Linkage, Euclidian distance", xlab="", sub="", cex=.9)
plot(hc.corr.complete,main="Complete Linkage, correlation-based distance", xlab="", sub="", cex=.9)
plot(hc.corr.average, main="Average Linkage, correlation-based distance", xlab="", sub="", cex=.9)
plot(hc.corr.single, main="Single Linkage, correlation-based distance", xlab="", sub="", cex=.9)
correlation<-as.dist(1-cor(t(GeneData)))
hc.corr.complete=hclust(correlation, method="complete")
hc.corr.average=hclust(correlation, method="average")
hc.corr.single=hclust(correlation, method="single")
par(mfrow=c(2,3))
plot(hc.eucl.complete,main="Complete Linkage, Euclidian distance", xlab="", sub="", cex=.9)
plot(hc.eucl.average, main="Average Linkage, Euclidian distance", xlab="", sub="", cex=.9)
plot(hc.eucl.single, main="Single Linkage, Euclidian distance", xlab="", sub="", cex=.9)
plot(hc.corr.complete,main="Complete Linkage, correlation-based distance", xlab="", sub="", cex=.9)
plot(hc.corr.average, main="Average Linkage, correlation-based distance", xlab="", sub="", cex=.9)
plot(hc.corr.single, main="Single Linkage, correlation-based distance", xlab="", sub="", cex=.9)
cutree(hc.eucl.complete, 2)
cutree(hc.eucl.complete, 2)
cutree(hc.eucl.average, 2)
cutree(hc.eucl.single, 2)
cutree(hc.corr.complete, 2)
cutree(hc.corr.average, 2)
cutree(hc.corr.single, 2)
pr.out=prcomp(GeneData, scale=TRUE)
plot(pr.out$x[,1:2], col=Cols(GeneData.labs), pch=19,xlab="Z1",ylab="Z2")
pr.out=prcomp(GeneData, scale=TRUE)
plot(pr.out$x[,1:2], col=GeneData.labs, pch=19,xlab="Z1",ylab="Z2")
pr.out=prcomp(GeneData, scale=TRUE)
plot(pr.out$x[,1:2], pch=19,xlab="Z1",ylab="Z2")
GeneData[1,]
GeneData[1,]
pr.out=prcomp(GeneData, scale=TRUE)
plot(pr.out$x[,1:2], col=GeneData[1:20,],pch=19,xlab="Z1",ylab="Z2")
c(0*10,1*10)
c(rep(1,20),rep(0,20))
color<-c(rep(1,20),rep(0,20))
pr.out=prcomp(GeneData, scale=TRUE)
plot(pr.out$x[,1:2], col=c("red","blue")*color,pch=19,xlab="Z1",ylab="Z2")
pr.out=prcomp(GeneData, scale=TRUE)
plot(pr.out$x[,1:2], col=c("red","blue")color,pch=19,xlab="Z1",ylab="Z2")
pr.out=prcomp(GeneData, scale=TRUE)
plot(pr.out$x[,1:2], col=c("red","blue")[color],pch=19,xlab="Z1",ylab="Z2")
color<-c(rep(1,20),rep(0,20))
pr.out=prcomp(GeneData, scale=TRUE)
plot(pr.out$x[,1:2], col=c("red","blue")[color],pch=19,xlab="Z1",ylab="Z2")
color<-c(rep(1,20),rep(2,20))
pr.out=prcomp(GeneData, scale=TRUE)
plot(pr.out$x[,1:2], col=c("red","blue")[color],pch=19,xlab="Z1",ylab="Z2")
color<-c(rep(1,20),rep(2,20))
pr.out=prcomp(GeneData, scale=TRUE)
plot(pr.out$x[,1:2], col=c("red","blue")[color],pch=19,xlab="Z1",ylab="Z2")
pve=100*pr.out$sdev^2/sum(pr.out$sdev^2)
par(mfrow=c(1,2))
plot(pve,  type="o", ylab="PVE", xlab="Principal Component", col="blue")
plot(cumsum(pve), type="o", ylab="Cumulative PVE", xlab="Principal Component", col="brown3")
pve
pve[5]
cumsum(pve)
cumsum(pve)[5]
pve=100*pr.out$sdev^2/sum(pr.out$sdev^2)
cumsum(pve)[5]
pr.out
summary(pr.out)
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=68),tidy=TRUE)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
cache=TRUE, size="scriptsize")
library(GGally)
library(gridExtra)
max(d.train$npreg)#max nr of pregnancies
id <- "1Fv6xwKLSZHldRAC1MrcK2mzdOYnbgv0E"  # google file ID
d.diabetes <- dget(sprintf("https://docs.google.com/uc?id=%s&export=download",
id))
d.train = d.diabetes$ctrain
d.test = d.diabetes$ctest
library(GGally)
library(gridExtra)
max(d.train$npreg)#max nr of pregnancies
head(d.train) #overview of data
#plots
ggpairs(d.train) + theme_minimal() #look at correlation between variables
plot2 <- ggplot(d.train, aes(x=npreg, y=diabetes))+geom_point()
plot3 <- ggplot(d.train,aes(x=glu,y=bmi,color=diabetes))+geom_point()
grid.arrange(plot2, plot3, ncol=2, nrow = 1)
#Fit a support vector classifier (linear boundary)
svmfit = svm(d.train$diabetes ~ ., data = dat.train, kernel = "linear", cost = 10, scale = FALSE)
library(e1071)
set.seed(10111)
#make response variable a factor
d.train$diabetes <- as.factor(d.train$diabetes)
d.test$diabetes <- as.factor(d.test$diabetes)
#data frame
dat.train = data.frame(x = d.train[, 1:length(dat.train)], y = d.train$diabetes)
library(e1071)
set.seed(10111)
#make response variable a factor
d.train$diabetes <- as.factor(d.train$diabetes)
d.test$diabetes <- as.factor(d.test$diabetes)
#data frame
dat.train = data.frame(x = d.train[, 1:length(d.train)], y = d.train$diabetes)
dat.test = data.frame(x = d.test[, 1:length(d.train)], y = d.test$diabetes)
#Fit a support vector classifier (linear boundary)
svmfit = svm(d.train$diabetes ~ ., data = dat.train, kernel = "linear", cost = 10, scale = FALSE)
plot(svmfit, dat.train, col = c("lightcoral", "lightgreen"))
library(e1071)
set.seed(10111)
#make response variable a factor
d.train$diabetes <- as.factor(d.train$diabetes)
d.test$diabetes <- as.factor(d.test$diabetes)
#data frame
dat.train = data.frame(x = d.train[, 1:length(d.train)], y = d.train$diabetes)
dat.test = data.frame(x = d.test[, 1:length(d.train)], y = d.test$diabetes)
#Fit a support vector classifier (linear boundary)
svmfit = svm(d.train$diabetes ~ ., data = dat.train, kernel = "linear", cost = 10, scale = FALSE)
plot(svmfit, dat.train)
library(e1071)
set.seed(10111)
#make response variable a factor
d.train$diabetes <- as.factor(d.train$diabetes)
d.test$diabetes <- as.factor(d.test$diabetes)
#data frame
dat.train = data.frame(x = d.train[, 1:length(d.train)], y = d.train$diabetes)
dat.test = data.frame(x = d.test[, 1:length(d.train)], y = d.test$diabetes)
#Fit a support vector classifier (linear boundary)
svmfit = svm(d.train$diabetes ~ ., data = dat.train, kernel = "linear", cost = 10, scale = FALSE)
plot(svmfit, data=dat.train)
?plot
library(e1071)
set.seed(10111)
#make response variable a factor
d.train$diabetes <- as.factor(d.train$diabetes)
d.test$diabetes <- as.factor(d.test$diabetes)
#data frame
dat.train = data.frame(x = d.train[, 1:length(d.train)], y = d.train$diabetes)
dat.test = data.frame(x = d.test[, 1:length(d.train)], y = d.test$diabetes)
#Fit a support vector classifier (linear boundary)
svmfit = svm(d.train$diabetes ~ ., data = dat.train, kernel = "linear", cost = 10, scale = FALSE)
plot(x=svmfit, y=dat.train)
plot(x=svmfit, y=dat.train)
svmfit
dat.train
d.train
d.train
d.train
library(e1071)
set.seed(10111)
#make response variable a factor
d.train$diabetes <- as.factor(d.train$diabetes)
d.test$diabetes <- as.factor(d.test$diabetes)
#data frame
#Fit a support vector classifier (linear boundary)
svmfit = svm(d.train$diabetes ~ ., data = d.train, kernel = "linear", cost = 10, scale = FALSE)
plot(x=svmfit, y=d.train)
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=68),tidy=TRUE)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
cache=TRUE, size="scriptsize")
library(ISLR)
library(leaps)
library(glmnet)
set.seed(1)
#make training and testing set
train.ind = sample(1:nrow(College), 0.5 * nrow(College))
college.train = College[train.ind, ]
college.test = College[-train.ind, ]
#the structure of the data
str(College)
nb_predictors<-17
forward<-regsubsets(Outstate~.,college.train,nvmax=17,method="forward")
sum<-summary(forward)
par(mfrow=c(1,2))
plot(sum$rss,xlab="Number of Variables",ylab="RSS",type="l")
plot(sum$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
nb_selected_pred<-5
variables<-names( coef( forward,id=nb_selected_pred ) )
variables
#fit the reduced model
reduced.model<-lm(Outstate~Private+Room.Board+Grad.Rate+perc.alumni+Expend, data =college.train)
summary(reduced.model)
#find test MSE
p<-predict(reduced.model,newdata=college.test)
mse_fwd <- mean(((college.test$Outstate)-p)^2)
mse_fwd
#Make a x matrix and y vector for both the training and testing set
x_train<-model.matrix(Outstate~.,college.train)[,-1]
y_train<-college.train$Outstate
x_test<-model.matrix(Outstate~.,college.test)[,-1]
y_test<-college.test$Outstate
set.seed(5555)
#perform the Lasso method and choose the best model using CV
lasso.mod = glmnet(x_train,y_train,alpha=1) #lasso method on train set
cv.lasso = cv.glmnet(x_train,y_train,alpha=1) #CV on train set
lambda.best = cv.lasso$lambda.min #select best lambda
lambda.best
#find the test MSE
predictions<-predict(lasso.mod,s=lambda.best,newx=x_test)
mse_lasso <- mean((predictions-y_test)^2) #test MSE
mse_lasso
c<-coef(lasso.mod,s=lambda.best,exact=TRUE)
inds<-which(c!=0)
variables<-row.names(c)[inds]
variables
ds1 = college.train[c("Private", "Outstate")] #binary variable
ds2 = college.train[c("Room.Board", "Outstate")]
ds3 = college.train[c("Terminal", "Outstate")]
ds4 = college.train[c("perc.alumni", "Outstate")]
ds5 = college.train[c("Expend", "Outstate")]
ds6 = college.train[c("Grad.Rate", "Outstate")]
par(mfrow=c(2,3))
plot(ds1)
plot(ds2)
plot(ds3)
plot(ds4)
plot(ds5)
plot(ds6)
library(ggplot2)
#make a dataframe
ds = College[c("Terminal", "Outstate")]
n = nrow(ds)
# chosen degrees
deg = 1:10
#now iterate over each degree d
dat = c()  #make a empty variable to store predicted values for each degree
MSE_poly = c(rep(0,10))  #make a empty variable to store MSE for each degree
for (d in deg) {
# fit model with this degree
mod = lm(Outstate ~ poly(Terminal, d), ds[train.ind, ])
#dataframe for Terminal and Outstate showing result for each degree over all samples
dat = rbind(dat, data.frame(Terminal = ds[train.ind, 1], Outstate = mod$fit,
degree = as.factor(rep(d,length(mod$fit)))))
# training MSE
MSE_poly[d] = mean((predict(mod, ds[-train.ind, ]) - ds[-train.ind, 2])^2)
}
# plot fitted values for different degrees
ggplot(data = ds[train.ind, ], aes(x = Terminal, y = Outstate)) +
geom_point(color = "darkgrey") + labs(title = "Polynomial regression")+
geom_line(data = dat, aes(x = Terminal, y = Outstate, color = degree))
library(splines)
#plot training set for Expend as only covariate
plot(college.train$Expend, college.train$Outstate, col = "darkgrey", main="Smoothing spline", xlab="Expend", ylab="Outstate")
#perform CV in order to find optimal number of df
fit = smooth.spline(college.train$Expend, college.train$Outstate, cv=TRUE)
df <- fit$df #choose df from CV
l <- fit$lambda
#add fitted function from smoothing spline
lines(fit, col="red", lwd=2)
legend("topright", legend=c("4.6 DF"), col="red", lty=1, lwd=2, cex=.8)
#training MSE
pred = predict(fit, newdata=college.train)
MSE_spline = mean( (college.train$Outstate - pred$y)^2 )
#MSE for polynomial regression models (1-10)
plot(1:10,MSE_poly, type = "o", pch = 16, xlab = "degree", main = "Test error")
min(MSE_poly)
#MSE for smoothing spline
MSE_spline
library(randomForest)
set.seed(1)
bag.college <- randomForest(Outstate ~ .,data=college.train, mtry=5, ntrees=500)
yhat.college <- predict(bag.college, newdata=college.test)
MSE_rf <- mean((yhat.college - college.test$Outstate)^2)
MSE_rf
MSE <- c(mse_fwd,mse_lasso,min(MSE_poly),MSE_spline,MSE_rf)
Method <- c("Forward selection", "Lasso", "Polynomial regression", "Smoothing spline",
"Random Forest")
df <- data.frame(Method, MSE)
kable(df)
id <- "1Fv6xwKLSZHldRAC1MrcK2mzdOYnbgv0E"  # google file ID
d.diabetes <- dget(sprintf("https://docs.google.com/uc?id=%s&export=download",
id))
d.train = d.diabetes$ctrain
d.test = d.diabetes$ctest
library(GGally)
library(gridExtra)
max(d.train$npreg)#max nr of pregnancies
head(d.train) #overview of data
#plots
ggpairs(d.train) + theme_minimal() #look at correlation between variables
plot2 <- ggplot(d.train, aes(x=npreg, y=diabetes))+geom_point()
plot3 <- ggplot(d.train,aes(x=glu,y=bmi,color=diabetes))+geom_point()
grid.arrange(plot2, plot3, ncol=2, nrow = 1)
library(e1071)
set.seed(10111)
#make response variable a factor
d.train$diabetes <- as.factor(d.train$diabetes)
d.test$diabetes <- as.factor(d.test$diabetes)
#data frame
#Fit a support vector classifier (linear boundary)
svmfit = svm(d.train$diabetes ~ ., data = d.train, kernel = "linear", cost = 10, scale = FALSE)
plot(x=svmfit, y=d.train)
