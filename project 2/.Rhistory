#fit a logistic regression model using training data
glm.fit <- glm(diabetes ~ .,data=d.train, family="binomial")
#predict the response using testing data
glm.pred <- predict(glm.fit, d.test, type="response")
glm.pred
t<-table(tennisTest$Result, probs)
colnames(t)=c("Predicted Loss","Predicted Win")
rownames(t)=c("Actual Loss","Actual Win")
t
table(Prediction = glm.pred, Truth = d.test$diabetes, type="response")
#fit a logistic regression model using training data
glm.fit <- glm(diabetes ~ .,data=d.train, family="binomial")
#predict the response using testing data
glm.pred <- predict(glm.fit, d.test)
glm.pred
t<-table(tennisTest$Result, probs)
colnames(t)=c("Predicted Loss","Predicted Win")
rownames(t)=c("Actual Loss","Actual Win")
t
table(Prediction = glm.pred, Truth = d.test$diabetes, type="response")
#fit a logistic regression model using training data
glm.fit <- glm(diabetes ~ .,data=d.train, family="binomial")
#predict the response using testing data
glm.pred <- predict(glm.fit, newdata=d.test, type="response")
glm.pred
t<-table(tennisTest$Result, probs)
colnames(t)=c("Predicted Loss","Predicted Win")
rownames(t)=c("Actual Loss","Actual Win")
t
table(Prediction = glm.pred, Truth = d.test$diabetes, type="response")
diabetes
plot(glm.fit)
glm.fit <- glm(diabetes ~ .,data=d.train, family="binomial")
#predict the response using testing data
glm.pred <- predict(glm.fit, newdata=d.test, type="response")
glm.pred
```{r, echo=TRUE, eval=TRUE}
glm.pred = rep("Down",1250) #create vector of 1250 down elements
glm.pred[glm.probs>0.5]="Up"
glm.probs <- predict(glm.fit, newdata=d.test, type="response")
#sort the probabilities for whether the observations are > or > than 0.5
glm.pred = rep("Down",1250) #create vector of 1250 down elements
glm.pred[glm.probs>0.5]="Up"
glm.probs <- predict(glm.fit, newdata=d.test, type="response")
#sort the probabilities for whether the observations are > or > than 0.5
glm.pred = rep("Down",1250) #create vector of 1250 down elements
glm.pred[glm.probs>0.5]="Up"
table(Prediction = glm.pred, Truth = d.test$diabetes)
length(glm.pred)
#fit a logistic regression model using training data
glm.fit <- glm(diabetes ~ .,data=d.train, family="binomial")
#predict the response using testing data
glm.probs <- predict(glm.fit, newdata=d.test, type="response")
#sort the probabilities for whether the observations are > or > than 0.5
glm.pred = rep("Down",length(d.test$diabetes)) #create vector of nr. of elements = dataset
glm.pred[glm.probs>0.5]="Up"
length(glm.pred)
table(Prediction = glm.pred, Truth = d.test$diabetes)
#fit a logistic regression model using training data
glm.fit <- glm(diabetes ~ .,data=d.train, family="binomial")
#predict the response using testing data
glm.probs <- predict(glm.fit, newdata=d.test, type="response")
#sort the probabilities for whether the observations are > or > than 0.5
glm.pred = rep("Down",length(d.test$diabetes)) #create vector of nr. of elements = dataset
glm.pred[glm.probs>0.5]="Up"
table(Prediction = glm.pred, Truth = d.test$diabetes)
#fit a logistic regression model using training data
glm.fit <- glm(diabetes ~ .,data=d.train, family="binomial")
#predict the response using testing data
glm.probs <- predict(glm.fit, newdata=d.test, type="response")
#sort the probabilities for whether the observations are > or > than 0.5
glm.pred = rep("0",length(d.test$diabetes)) #create vector of nr. of elements = dataset
glm.pred[glm.probs>0.5]="1"
table(Prediction = glm.pred, Truth = d.test$diabetes)
(32+19)/(136+32+19+45)
library(ggplot2)
#make a dataframe
ds = College[c("Terminal", "Outstate")]
n = nrow(ds)
# chosen degrees
deg = 1:10
#now iterate over each degree d
dat = c()  #make a empty variable to store predicted values for each degree
MSE_train_poly = c(rep(0,10))  #make a empty variable to store MSE for each degree
MSE_test_poly = c(rep(0,10))
for (d in deg) {
# fit model with this degree
mod = lm(Outstate ~ poly(Terminal, d), ds[train.ind, ])
#dataframe for Terminal and Outstate showing result for each degree over all samples
dat = rbind(dat, data.frame(Terminal = ds[train.ind, 1], Outstate = mod$fit,
degree = as.factor(rep(d,length(mod$fit)))))
# training MSE
MSE_train_poly[d] = mean((predict(mod, ds[-train.ind, ]) - ds[-train.ind, 2])^2)
MSE_test_poly[d] = mean((predict(mod, ds[-test.ind, ]) - ds[-test.ind, 2])^2)
}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=68),tidy=TRUE)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
cache=TRUE, size="scriptsize")
library(ISLR)
library(leaps)
library(glmnet)
set.seed(1)
#make training and testing set
train.ind = sample(1:nrow(College), 0.5 * nrow(College))
college.train = College[train.ind, ]
college.test = College[-train.ind, ]
#the structure of the data
str(College)
nb_predictors<-17
forward<-regsubsets(Outstate~.,college.train,nvmax=17,method="forward")
sum<-summary(forward)
par(mfrow=c(1,2))
plot(sum$rss,xlab="Number of Variables",ylab="RSS",type="l")
plot(sum$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
nb_selected_pred<-5
variables<-names( coef( forward,id=nb_selected_pred ) )
variables
#fit the reduced model
reduced.model<-lm(Outstate~Private+Room.Board+Grad.Rate+perc.alumni+Expend, data =college.train)
summary(reduced.model)
#find test MSE
p<-predict(reduced.model,newdata=college.test)
mse_fwd <- mean(((college.test$Outstate)-p)^2)
mse_fwd
#Make a x matrix and y vector for both the training and testing set
x_train<-model.matrix(Outstate~.,college.train)[,-1]
y_train<-college.train$Outstate
x_test<-model.matrix(Outstate~.,college.test)[,-1]
y_test<-college.test$Outstate
set.seed(5555)
#perform the Lasso method and choose the best model using CV
lasso.mod = glmnet(x_train,y_train,alpha=1) #lasso method on train set
cv.lasso = cv.glmnet(x_train,y_train,alpha=1) #CV on train set
lambda.best = cv.lasso$lambda.min #select best lambda
lambda.best
#find the test MSE
predictions<-predict(lasso.mod,s=lambda.best,newx=x_test)
mse_lasso <- mean((predictions-y_test)^2) #test MSE
mse_lasso
c<-coef(lasso.mod,s=lambda.best,exact=TRUE)
inds<-which(c!=0)
variables<-row.names(c)[inds]
variables
ds1 = college.train[c("Private", "Outstate")] #binary variable
ds2 = college.train[c("Room.Board", "Outstate")]
ds3 = college.train[c("Terminal", "Outstate")]
ds4 = college.train[c("perc.alumni", "Outstate")]
ds5 = college.train[c("Expend", "Outstate")]
ds6 = college.train[c("Grad.Rate", "Outstate")]
par(mfrow=c(2,3))
plot(ds1)
plot(ds2)
plot(ds3)
plot(ds4)
plot(ds5)
plot(ds6)
library(ggplot2)
#make a dataframe
ds = College[c("Terminal", "Outstate")]
n = nrow(ds)
# chosen degrees
deg = 1:10
#now iterate over each degree d
dat = c()  #make a empty variable to store predicted values for each degree
MSE_train_poly = c(rep(0,10))  #make a empty variable to store MSE for each degree
MSE_test_poly = c(rep(0,10))
for (d in deg) {
# fit model with this degree
mod = lm(Outstate ~ poly(Terminal, d), ds[train.ind, ])
#dataframe for Terminal and Outstate showing result for each degree over all samples
dat = rbind(dat, data.frame(Terminal = ds[train.ind, 1], Outstate = mod$fit,
degree = as.factor(rep(d,length(mod$fit)))))
# training MSE
MSE_train_poly[d] = mean((predict(mod, ds[-train.ind, ]) - ds[-train.ind, 2])^2)
MSE_test_poly[d] = mean((predict(mod, ds[-test.ind, ]) - ds[-test.ind, 2])^2)
}
library(ggplot2)
#make a dataframe
ds = College[c("Terminal", "Outstate")]
n = nrow(ds)
# chosen degrees
deg = 1:10
#now iterate over each degree d
dat = c()  #make a empty variable to store predicted values for each degree
MSE_train_poly = c(rep(0,10))  #make a empty variable to store MSE for each degree
MSE_test_poly = c(rep(0,10))
for (d in deg) {
# fit model with this degree
mod = lm(Outstate ~ poly(Terminal, d), data= college.train)
#dataframe for Terminal and Outstate showing result for each degree over all samples
dat = rbind(dat, data.frame(Terminal = college.train$Terminal, Outstate = mod$fit,
degree = as.factor(rep(d,length(mod$fit)))))
# training MSE
MSE_train_poly[d] = mean((predict(mod, newdata=college.train) - college.train$Outstate)^2)
MSE_test_poly[d] = mean((predict(mod, newdata= college.test) - college.test$Outstate)^2)
}
# plot fitted values for different degrees
ggplot(data = ds[train.ind, ], aes(x = Terminal, y = Outstate)) +
geom_point(color = "darkgrey") + labs(title = "Polynomial regression")+
geom_line(data = dat, aes(x = Terminal, y = Outstate, color = degree))
MSE_train_poly
MSE_test_poly
min(MSE_train_poly)
min(MSE_test_poly)
#MSE for polynomial regression models (1-10)
plot(1:10,MSE_train_poly, type = "o", pch = 16, xlab = "degree", main = "Test error")
min(MSE_train_poly)
#training MSE for smoothing spline
MSE_spline
library(ggplot2)
#make a dataframe
ds = College[c("Terminal", "Outstate")]
n = nrow(ds)
# chosen degrees
deg = 1:10
#now iterate over each degree d
dat = c()  #make a empty variable to store predicted values for each degree
MSE_train_poly = c(rep(0,10))  #make a empty variable to store MSE for each degree
MSE_test_poly = c(rep(0,10))
for (d in deg) {
# fit model with this degree
mod = lm(Outstate ~ poly(Terminal, d), data= college.train)
#dataframe for Terminal and Outstate showing result for each degree over all samples
dat = rbind(dat, data.frame(Terminal = college.train$Terminal, Outstate = mod$fit,
degree = as.factor(rep(d,length(mod$fit)))))
# training MSE
MSE_train_poly[d] = mean((predict(mod, newdata=college.train) - college.train$Outstate)^2)
MSE_test_poly[d] = mean((predict(mod, newdata= college.test) - college.test$Outstate)^2)
}
# plot fitted values for different degrees
ggplot(data = ds[train.ind, ], aes(x = Terminal, y = Outstate)) +
geom_point(color = "darkgrey") + labs(title = "Polynomial regression")+
geom_line(data = dat, aes(x = Terminal, y = Outstate, color = degree))
#MSE for polynomial regression models (1-10)
plot(1:10,MSE_train_poly, type = "o", pch = 16, xlab = "degree", main = "Test error")
min(MSE_train_poly)
#training MSE for smoothing spline
MSE_spline
MSE <- c(mse_fwd,mse_lasso,min(MSE_test_poly),MSE_test_spline,MSE_rf)
#training MSE for smoothing spline
MSE_spline_test
#training MSE for smoothing spline
MSE_spline_test
library(splines)
#plot training set for Expend as only covariate
plot(college.train$Expend, college.train$Outstate, col = "darkgrey",
main="Smoothing spline", xlab="Expend", ylab="Outstate")
#perform CV in order to find optimal number of df
fit = smooth.spline(college.train$Expend, college.train$Outstate, cv=TRUE)
df <- fit$df #choose df from CV
l <- fit$lambda
#add fitted function from smoothing spline
lines(fit, col="red", lwd=2)
legend("topright", legend=c("4.6 DF"), col="red", lty=1, lwd=2, cex=.8)
#training MSE
pred = predict(fit, newdata=college.train)
MSE_spline_train = mean( (college.train$Outstate - pred$y)^2 )
#test MSE
pred = predict(fit, newdata=college.test)
MSE_spline_test = mean( (college.test$Outstate - pred$y)^2 )
#MSE for polynomial regression models (1-10)
plot(1:10,MSE_train_poly, type = "o", pch = 16, xlab = "degree", main = "Test error")
MSE_poly_train_best = MSE_spline_train[9]
#training MSE for smoothing spline
MSE_spline_test
MSE <- c(mse_fwd,mse_lasso,min(MSE_test_poly),MSE_test_spline,MSE_rf)
MSE <- c(mse_fwd,mse_lasso,min(MSE_test_poly),MSE_spline_test,MSE_rf)
Method <- c("Forward selection", "Lasso", "Polynomial regression", "Smoothing spline",
"Random Forest")
df <- data.frame(Method, MSE)
kable(df)
?ggpot
?ggplot
library(GGally)
library(gridExtra)
max(d.train$npreg)#max nr of pregnancies
head(d.train) #overview of data
#plots
ggpairs(d.train) + theme_minimal() #look at correlation between variables
#plot2 <- ggplot(d.train, aes(x=npreg, y=diabetes))+geom_point()
plot2 <- ggplot(d.train, aes(npreg)) + geom_histogram(binwidth = 2) + labs(title = "Histogram") +
theme_minimal()
plot3 <- ggplot(d.train,aes(x=glu,y=bmi,color=diabetes))+geom_point()
grid.arrange(plot2, plot3, ncol=2, nrow = 1)
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=68),tidy=TRUE)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
cache=TRUE, size="scriptsize")
degree <- seq(1, 10, by=1)
df <- data.frame(degree, MSE_train_poly, )
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=68),tidy=TRUE)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
cache=TRUE, size="scriptsize")
library(ISLR)
library(leaps)
library(glmnet)
set.seed(1)
#make training and testing set
train.ind = sample(1:nrow(College), 0.5 * nrow(College))
college.train = College[train.ind, ]
college.test = College[-train.ind, ]
#the structure of the data
str(College)
nb_predictors<-17
forward<-regsubsets(Outstate~.,college.train,nvmax=17,method="forward")
sum<-summary(forward)
par(mfrow=c(1,2))
plot(sum$rss,xlab="Number of Variables",ylab="RSS",type="l")
plot(sum$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
nb_selected_pred<-5
variables<-names( coef( forward,id=nb_selected_pred ) )
variables
#fit the reduced model
reduced.model<-lm(Outstate~Private+Room.Board+Grad.Rate+perc.alumni+Expend, data =college.train)
summary(reduced.model)
#find test MSE
p<-predict(reduced.model,newdata=college.test)
mse_fwd <- mean(((college.test$Outstate)-p)^2)
mse_fwd
#Make a x matrix and y vector for both the training and testing set
x_train<-model.matrix(Outstate~.,college.train)[,-1]
y_train<-college.train$Outstate
x_test<-model.matrix(Outstate~.,college.test)[,-1]
y_test<-college.test$Outstate
set.seed(5555)
#perform the Lasso method and choose the best model using CV
lasso.mod = glmnet(x_train,y_train,alpha=1) #lasso method on train set
cv.lasso = cv.glmnet(x_train,y_train,alpha=1) #CV on train set
lambda.best = cv.lasso$lambda.min #select best lambda
lambda.best
#find the test MSE
predictions<-predict(lasso.mod,s=lambda.best,newx=x_test)
mse_lasso <- mean((predictions-y_test)^2) #test MSE
mse_lasso
c<-coef(lasso.mod,s=lambda.best,exact=TRUE)
inds<-which(c!=0)
variables<-row.names(c)[inds]
variables
ds1 = college.train[c("Private", "Outstate")] #binary variable
ds2 = college.train[c("Room.Board", "Outstate")]
ds3 = college.train[c("Terminal", "Outstate")]
ds4 = college.train[c("perc.alumni", "Outstate")]
ds5 = college.train[c("Expend", "Outstate")]
ds6 = college.train[c("Grad.Rate", "Outstate")]
par(mfrow=c(2,3))
plot(ds1)
plot(ds2)
plot(ds3)
plot(ds4)
plot(ds5)
plot(ds6)
library(ggplot2)
#make a dataframe
ds = College[c("Terminal", "Outstate")]
n = nrow(ds)
# chosen degrees
deg = 1:10
#now iterate over each degree d
dat = c()  #make a empty variable to store predicted values for each degree
MSE_train_poly = c(rep(0,10))  #make a empty variable to store MSE for each degree
MSE_test_poly = c(rep(0,10))
for (d in deg) {
# fit model with this degree
mod = lm(Outstate ~ poly(Terminal, d), data= college.train)
#dataframe for Terminal and Outstate showing result for each degree over all samples
dat = rbind(dat, data.frame(Terminal = college.train$Terminal, Outstate = mod$fit,
degree = as.factor(rep(d,length(mod$fit)))))
# training MSE
MSE_train_poly[d] = mean((predict(mod, newdata=college.train) - college.train$Outstate)^2)
MSE_test_poly[d] = mean((predict(mod, newdata= college.test) - college.test$Outstate)^2)
}
# plot fitted values for different degrees
ggplot(data = ds[train.ind, ], aes(x = Terminal, y = Outstate)) +
geom_point(color = "darkgrey") + labs(title = "Polynomial regression")+
geom_line(data = dat, aes(x = Terminal, y = Outstate, color = degree))
library(splines)
#plot training set for Expend as only covariate
plot(college.train$Expend, college.train$Outstate, col = "darkgrey",
main="Smoothing spline", xlab="Expend", ylab="Outstate")
#perform CV in order to find optimal number of df
fit = smooth.spline(college.train$Expend, college.train$Outstate, cv=TRUE)
df <- fit$df #choose df from CV
l <- fit$lambda
#add fitted function from smoothing spline
lines(fit, col="red", lwd=2)
legend("topright", legend=c("4.6 DF"), col="red", lty=1, lwd=2, cex=.8)
#training MSE
#pred = predict(fit, newdata=college.train$)
#MSE_spline_train = mean(predict(fit,x)(college.train$Outstate - pred$y)^2 )
pred =  predict( fit, college.train$Expend)
MSE_spline_train <- mean((pred$y - college.train$Outstate )^2)
#test MSE
pred = predict(fit, newdata=college.test)
MSE_spline_test = mean( (college.test$Outstate - pred$y)^2 )
#MSE for polynomial regression models (1-10)
#plot(1:10,MSE_train_poly, type = "o", pch = 16, xlab = "degree", main = "Test error")
Degree <- seq(1,10,by=1)
MSE = MSE_train_poly
df <- data.frame(Degree, MSE)
kable(df)
library(randomForest)
set.seed(1)
# fit a model using random forests with a sufficiently large number of trees
rf.college <- randomForest(Outstate ~ .,data=college.train, mtry=5, ntrees=500)
#predict the response using test data
yhat.college <- predict(rf.college, newdata=college.test)
#test MSE
MSE_rf <- mean((yhat.college - college.test$Outstate)^2)
MSE <- c(mse_fwd,mse_lasso,min(MSE_test_poly),MSE_spline_test,MSE_rf)
Method <- c("Forward selection", "Lasso", "Polynomial regression", "Smoothing spline",
"Random Forest")
df <- data.frame(Method, MSE)
kable(df)
id <- "1Fv6xwKLSZHldRAC1MrcK2mzdOYnbgv0E"  # google file ID
d.diabetes <- dget(sprintf("https://docs.google.com/uc?id=%s&export=download",
id))
d.train = d.diabetes$ctrain
d.test = d.diabetes$ctest
library(GGally)
library(gridExtra)
max(d.train$npreg)#max nr of pregnancies
head(d.train) #overview of data
#plots
ggpairs(d.train) + theme_minimal() #look at correlation between variables
#plot2 <- ggplot(d.train, aes(x=npreg, y=diabetes))+geom_point()
plot2 <- ggplot(d.train, aes(npreg)) + geom_histogram(binwidth = 2) + labs(title = "Histogram") +
theme_minimal()
plot3 <- ggplot(d.train,aes(x=glu,y=bmi,color=diabetes))+geom_point()
grid.arrange(plot2, plot3, ncol=2, nrow = 1)
library(e1071)
set.seed(10111)
d.train$diabetes <- as.factor(d.train$diabetes)
d.test$diabetes <- as.factor(d.test$diabetes)
#Fit a support vector classifier (linear boundary)
svm.linear = svm(diabetes~.,
data = d.train,
kernel = 'linear')
#fit a support vector machine (radial boundary)
svm.radial = svm(diabetes~.,
data = d.train,
kernel = 'radial')
#CV to find the best parameters for each model
cv.linear <- tune(svm, diabetes ~ ., data=d.train, kernel = "linear",
ranges = list(cost = c(.001, .01 , .1, 1, 10, 100 ) ) )
cv.radial <- tune(svm, diabetes ~., data = d.train, kernel = "radial",
ranges = list(cost = c(0.1,1,10,100,1000), gamma = c(0.5,1,2,3,4) ))
#fit new models with the optimalized parameters from CV
bestmod.linear = cv.linear$best.model
bestmod.radial = cv.radial$best.model
# Predict the response for the test set
pred.lin = predict(bestmod.linear, newdata = d.test)
pred.rad = predict(bestmod.radial, newdata = d.test)
# Confusion tables (0: no diabetes, 1: diabetes)
#for SVC (linear)
table(Prediction = pred.lin, Truth = d.test$diabetes)
#for SVM (radial)
table(Prediction = pred.rad, Truth = d.test$diabetes)
#fit a logistic regression model using training data
glm.fit <- glm(diabetes ~ .,data=d.train, family="binomial")
#predict the response using testing data
glm.probs <- predict(glm.fit, newdata=d.test, type="response")
#sort the probabilities for whether the observations are < or > than p = 0.5
glm.pred = rep("0",length(d.test$diabetes)) #create vector of nr. of elements = dataset
glm.pred[glm.probs>0.5]="1"
#confusion table
table(Prediction = glm.pred, Truth = d.test$diabetes)
id <- "1VfVCQvWt121UN39NXZ4aR9Dmsbj-p9OU"  # google file ID
GeneData <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id), header = F)
colnames(GeneData)[1:20] = paste(rep("H", 20), c(1:20), sep = "")
colnames(GeneData)[21:40] = paste(rep("D", 20), c(1:20), sep = "")
row.names(GeneData) = paste(rep("G", 1000), c(1:1000), sep = "")
MSE_spline_train
library(splines)
#perform CV in order to find optimal number of df
fit = smooth.spline(college.train$Expend, college.train$Outstate, cv=TRUE)
df <- fit$df #choose df from CV
l <- fit$lambda
#plot training set for Expend as only covariate
plot(college.train$Expend, college.train$Outstate, col = "darkgrey",
main="Smoothing spline, df = ", round(fit$df,3), xlab="Expend", ylab="Outstate")
library(splines)
#perform CV in order to find optimal number of df
fit = smooth.spline(college.train$Expend, college.train$Outstate, cv=TRUE)
df <- fit$df #choose df from CV
l <- fit$lambda
#plot training set for Expend as only covariate
plot(college.train$Expend, college.train$Outstate, col = "darkgrey",
main=paste("Smoothing spline, df = ", round(fit$df,3)), xlab="Expend", ylab="Outstate")
#add fitted function from smoothing spline
lines(fit, col="red", lwd=2)
#training MSE
pred =  predict( fit, college.train$Expend)
MSE_spline_train <- mean((pred$y - college.train$Outstate )^2)
#test MSE
pred =  predict(fit, college.test$Expend)
MSE_spline_test = mean( (college.test$Outstate - pred$y)^2 )
library(splines)
#perform CV in order to find optimal number of df
fit = smooth.spline(college.train$Expend, college.train$Outstate, cv=TRUE)
#plot training set for Expend as only covariate
plot(college.train$Expend, college.train$Outstate, col = "darkgrey",
main=paste("Smoothing spline, df = ", round(fit$df,3)), xlab="Expend", ylab="Outstate")
#add fitted function from smoothing spline
lines(fit, col="red", lwd=2)
#training MSE
pred =  predict( fit, college.train$Expend)
MSE_spline_train <- mean((pred$y - college.train$Outstate )^2)
#test MSE
pred =  predict(fit, college.test$Expend)
MSE_spline_test = mean( (college.test$Outstate - pred$y)^2 )
