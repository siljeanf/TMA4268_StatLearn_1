summary(reduced.model)
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=68),tidy=TRUE)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
cache=TRUE, size="scriptsize")
library(ISLR)
library(leaps)
library(glmnet)
set.seed(1)
#make training and testing set
train.ind = sample(1:nrow(College), 0.5 * nrow(College))
college.train = College[train.ind, ]
college.test = College[-train.ind, ]
#the structure of the data
str(College)
nb_predictors<-17
forward<-regsubsets(Outstate~.,college.train,nvmax=17,method="forward")
sum<-summary(forward)
par(mfrow=c(1,2))
plot(sum$rss,xlab="Number of Variables",ylab="RSS",type="l")
plot(sum$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
nb_selected_pred<-5
variables<-names( coef( forward,id=nb_selected_pred ) )
variables
#fit the reduced model
reduced.model<-lm(Outstate~Private+Room.Board+Grad.Rate+perc.alumni+Expend, data =college.train)
summary(reduced.model)
#find test MSE
p<-predict(reduced.model,newdata=college.test)
error1 <- mean(((college.test$Outstate)-p)^2)
error1
#Make a x matrix and y vector for both the training and testing set
x_train<-model.matrix(Outstate~.,college.train)[,-1]
y_train<-college.train$Outstate
x_test<-model.matrix(Outstate~.,college.test)[,-1]
y_test<-college.test$Outstate
set.seed(2)
#perform the Lasso method and choose the best model using CV
lasso.mod = glmnet(x_train,y_train,alpha=1) #lasso method on train set
cv.lasso = cv.glmnet(x_train,y_train,alpha=1) #CV on train set
lambda.best = cv.lasso$lambda.min #select best lambda
lambda.best
#do we need this plot ???
plot(cv.lasso)
#find the test MSE
predictions<-predict(lasso.mod,s=lambda.best,newx=x_test)
error2 <- mean((predictions-y_test)^2) #test MSE
error2
c<-coef(lasso.mod,s=lambda.best,exact=TRUE)
inds<-which(c!=0)
variables<-row.names(c)[inds]
variables
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=68),tidy=TRUE)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
cache=TRUE, size="scriptsize")
library(ISLR)
library(leaps)
library(glmnet)
set.seed(1)
#make training and testing set
train.ind = sample(1:nrow(College), 0.5 * nrow(College))
college.train = College[train.ind, ]
college.test = College[-train.ind, ]
#the structure of the data
str(College)
nb_predictors<-17
forward<-regsubsets(Outstate~.,college.train,nvmax=17,method="forward")
sum<-summary(forward)
par(mfrow=c(1,2))
plot(sum$rss,xlab="Number of Variables",ylab="RSS",type="l")
plot(sum$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
nb_selected_pred<-5
variables<-names( coef( forward,id=nb_selected_pred ) )
variables
#fit the reduced model
reduced.model<-lm(Outstate~Private+Room.Board+Grad.Rate+perc.alumni+Expend, data =college.train)
summary(reduced.model)
#find test MSE
p<-predict(reduced.model,newdata=college.test)
error1 <- mean(((college.test$Outstate)-p)^2)
error1
#Make a x matrix and y vector for both the training and testing set
x_train<-model.matrix(Outstate~.,college.train)[,-1]
y_train<-college.train$Outstate
x_test<-model.matrix(Outstate~.,college.test)[,-1]
y_test<-college.test$Outstate
set.seed(2)
#perform the Lasso method and choose the best model using CV
lasso.mod = glmnet(x_train,y_train,alpha=1) #lasso method on train set
cv.lasso = cv.glmnet(x_train,y_train,alpha=1) #CV on train set
lambda.best = cv.lasso$lambda.min #select best lambda
lambda.best
#do we need this plot ???
plot(cv.lasso)
#find the test MSE
predictions<-predict(lasso.mod,s=lambda.best,newx=x_test)
error2 <- mean((predictions-y_test)^2) #test MSE
error2
c<-coef(lasso.mod,s=lambda.best,exact=TRUE)
inds<-which(c!=0)
variables<-row.names(c)[inds]
variables
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=68),tidy=TRUE)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
cache=TRUE, size="scriptsize")
library(ISLR)
library(leaps)
library(glmnet)
set.seed(1)
#make training and testing set
train.ind = sample(1:nrow(College), 0.5 * nrow(College))
college.train = College[train.ind, ]
college.test = College[-train.ind, ]
#the structure of the data
str(College)
nb_predictors<-17
forward<-regsubsets(Outstate~.,college.train,nvmax=17,method="forward")
sum<-summary(forward)
par(mfrow=c(1,2))
plot(sum$rss,xlab="Number of Variables",ylab="RSS",type="l")
plot(sum$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
nb_selected_pred<-5
variables<-names( coef( forward,id=nb_selected_pred ) )
variables
#fit the reduced model
reduced.model<-lm(Outstate~Private+Room.Board+Grad.Rate+perc.alumni+Expend, data =college.train)
summary(reduced.model)
#find test MSE
p<-predict(reduced.model,newdata=college.test)
error1 <- mean(((college.test$Outstate)-p)^2)
error1
#Make a x matrix and y vector for both the training and testing set
x_train<-model.matrix(Outstate~.,college.train)[,-1]
y_train<-college.train$Outstate
x_test<-model.matrix(Outstate~.,college.test)[,-1]
y_test<-college.test$Outstate
set.seed(2)
#perform the Lasso method and choose the best model using CV
lasso.mod = glmnet(x_train,y_train,alpha=1) #lasso method on train set
cv.lasso = cv.glmnet(x_train,y_train,alpha=1) #CV on train set
lambda.best = cv.lasso$lambda.min #select best lambda
lambda.best
#do we need this plot ???
plot(cv.lasso)
#find the test MSE
predictions<-predict(lasso.mod,s=lambda.best,newx=x_test)
error2 <- mean((predictions-y_test)^2) #test MSE
error2
c<-coef(lasso.mod,s=lambda.best,exact=TRUE)
inds<-which(c!=0)
variables<-row.names(c)[inds]
variables
set.seed(5555)
#perform the Lasso method and choose the best model using CV
lasso.mod = glmnet(x_train,y_train,alpha=1) #lasso method on train set
cv.lasso = cv.glmnet(x_train,y_train,alpha=1) #CV on train set
lambda.best = cv.lasso$lambda.min #select best lambda
lambda.best
#find the test MSE
predictions<-predict(lasso.mod,s=lambda.best,newx=x_test)
error2 <- mean((predictions-y_test)^2) #test MSE
error2
set.seed(5555)
#perform the Lasso method and choose the best model using CV
lasso.mod = glmnet(x_train,y_train,alpha=1) #lasso method on train set
cv.lasso = cv.glmnet(x_train,y_train,alpha=1) #CV on train set
lambda.best = cv.lasso$lambda.min #select best lambda
lambda.best
#find the test MSE
predictions<-predict(lasso.mod,s=lambda.best,newx=x_test)
error2 <- mean((predictions-y_test)^2) #test MSE
error2
plot(Outstate)
ds1 = college.train[c("Outstate", "Private")] #qualitative variable ???
ds2 = college.train[c("Outstate", "Room.Board")]
ds3 = college.train[c("Outstate", "Terminal")]
ds4 = college.train[c("Outstate", "perc.alumni")]
ds5 = college.train[c("Outstate", "Expend")]
ds6 = college.train[c("Outstate", "Grad.Rate")]
par(mfrow=c(4,2))
plot(ds1)
plot(ds2)
plot(ds3)
plot(ds4)
plot(ds5)
plot(ds6)
ds1 = college.train[c("Outstate", "Private")] #qualitative variable ???
ds2 = college.train[c("Outstate", "Room.Board")]
ds3 = college.train[c("Outstate", "Terminal")]
ds4 = college.train[c("Outstate", "perc.alumni")]
ds5 = college.train[c("Outstate", "Expend")]
ds6 = college.train[c("Grad.Rate", "Outstate")]
par(mfrow=c(2,3))
plot(ds1)
plot(ds2)
plot(ds3)
plot(ds4)
plot(ds5)
plot(ds6)
ds1 = college.train[c("Private", "Outstate")] #binary variable
ds2 = college.train[c("Room.Board", "Outstate")]
ds3 = college.train[c("Terminal", "Outstate")]
ds4 = college.train[c("perc.alumni", "Outstate")]
ds5 = college.train[c("Expend", "Outstate")]
ds6 = college.train[c("Grad.Rate", "Outstate")]
par(mfrow=c(2,3))
plot(ds1)
plot(ds2)
plot(ds3)
plot(ds4)
plot(ds5)
plot(ds6)
#make a dataframe
ds = college.train[c("Terminal", "Outstate")]
n = nrow(ds)
# which degrees we will look at
deg = 1:10
# plot of training data
plot(ds[train.ind, ], col = "darkgrey", main = "Polynomial regression")
#make a dataframe
ds = College[c("Terminal", "Outstate")]
n = nrow(ds)
# chosen degrees
deg = 1:10
# plot of training data
plot(ds[train.ind, ], col = "darkgrey", main = "Polynomial regression")
colors = rainbow(length(deg))
# iterate over all degrees (1:4) - could also use a for-loop here
MSE = sapply(deg, function(d) {
# fit model with this degree
mod = lm(Outstate ~ poly(Terminal, d), ds[train.ind, ])
# add lines to the plot
lines(cbind(ds[train.ind, 1], mod$fit)[order(ds[traind.ind, 1]), ], col = co[d])
# calculate mean MSE - this is returned in the MSE variable
mean((predict(mod, ds[-train.ind, ]) - ds[-train.ind, 2])^2)
})
#make a dataframe
ds = College[c("Terminal", "Outstate")]
n = nrow(ds)
# chosen degrees
deg = 1:10
# plot of training data
plot(ds[train.ind, ], col = "darkgrey", main = "Polynomial regression")
colors = rainbow(length(deg))
# iterate over all degrees (1:4) - could also use a for-loop here
MSE = sapply(deg, function(d) {
# fit model with this degree
mod = lm(Outstate ~ poly(Terminal, d), ds[train.ind, ])
# add lines to the plot
lines(cbind(ds[train.ind, 1], mod$fit)[order(ds[train.ind, 1]), ], col = co[d])
# calculate mean MSE - this is returned in the MSE variable
mean((predict(mod, ds[-train.ind, ]) - ds[-train.ind, 2])^2)
})
#make a dataframe
ds = College[c("Terminal", "Outstate")]
n = nrow(ds)
# chosen degrees
deg = 1:10
# plot of training data
plot(ds[train.ind, ], col = "darkgrey", main = "Polynomial regression")
colors = rainbow(length(deg))
# iterate over all degrees (1:4) - could also use a for-loop here
MSE = sapply(deg, function(d) {
# fit model with this degree
mod = lm(Outstate ~ poly(Terminal, d), ds[train.ind, ])
# add lines to the plot
lines(cbind(ds[train.ind, 1], mod$fit)[order(ds[train.ind, 1]), ], col = colors[d])
# calculate mean MSE - this is returned in the MSE variable
mean((predict(mod, ds[-train.ind, ]) - ds[-train.ind, 2])^2)
})
# add legend to see which color corresponds to which line
legend("topright", legend = paste("d =", deg), lty = 1, col = co)
#make a dataframe
ds = College[c("Terminal", "Outstate")]
n = nrow(ds)
# chosen degrees
deg = 1:10
# plot of training data
plot(ds[train.ind, ], col = "darkgrey", main = "Polynomial regression")
colors = rainbow(length(deg))
# iterate over all degrees (1:4) - could also use a for-loop here
MSE = sapply(deg, function(d) {
# fit model with this degree
mod = lm(Outstate ~ poly(Terminal, d), ds[train.ind, ])
# add lines to the plot
lines(cbind(ds[train.ind, 1], mod$fit)[order(ds[train.ind, 1]), ], col = colors[d])
# calculate mean MSE - this is returned in the MSE variable
mean((predict(mod, ds[-train.ind, ]) - ds[-train.ind, 2])^2)
})
# add legend to see which color corresponds to which line
legend("topright", legend = paste("d =", deg), lty = 1, col = colors)
#make a dataframe
ds = College[c("Terminal", "Outstate")]
n = nrow(ds)
# chosen degrees
deg = 1:10
# plot of training data
plot(ds[train.ind, ], col = "darkgrey", main = "Polynomial regression")
colors = rainbow(length(deg))
# iterate over all degrees (1:4) - could also use a for-loop here
MSE = sapply(deg, function(d) {
# fit model with this degree
mod = lm(Outstate ~ poly(Terminal, d), ds[train.ind, ])
# add lines to the plot
lines(cbind(ds[train.ind, 1], mod$fit)[order(ds[train.ind, 1]), ], col = colors[d])
# calculate mean MSE - this is returned in the MSE variable
mean((predict(mod, ds[-train.ind, ]) - ds[-train.ind, 2])^2)
})
# add legend to see which color corresponds to which line
legend("top", legend = paste("d =", deg), lty = 1, col = colors)
?legend
#make a dataframe
ds = College[c("Terminal", "Outstate")]
n = nrow(ds)
# chosen degrees
deg = 1:10
# plot of training data
plot(ds[train.ind, ], col = "darkgrey", main = "Polynomial regression")
colors = rainbow(length(deg))
# iterate over all degrees (1:4) - could also use a for-loop here
MSE = sapply(deg, function(d) {
# fit model with this degree
mod = lm(Outstate ~ poly(Terminal, d), ds[train.ind, ])
# add lines to the plot
lines(cbind(ds[train.ind, 1], mod$fit)[order(ds[train.ind, 1]), ], col = colors[d])
# calculate mean MSE - this is returned in the MSE variable
mean((predict(mod, ds[-train.ind, ]) - ds[-train.ind, 2])^2)
})
# add legend to see which color corresponds to which line
par(c(1,1), xpd=TRUE)
legend("topright", legend = paste("d =", deg), lty = 1, col = colors)
?legend
#make a dataframe
ds = College[c("Terminal", "Outstate")]
n = nrow(ds)
# chosen degrees
deg = 1:10
# plot of training data
plot(ds[train.ind, ], col = "darkgrey", main = "Polynomial regression")
colors = rainbow(length(deg))
# iterate over all degrees (1:4) - could also use a for-loop here
MSE = sapply(deg, function(d) {
# fit model with this degree
mod = lm(Outstate ~ poly(Terminal, d), ds[train.ind, ])
# add lines to the plot
lines(cbind(ds[train.ind, 1], mod$fit)[order(ds[train.ind, 1]), ], col = colors[d])
# calculate mean MSE - this is returned in the MSE variable
mean((predict(mod, ds[-train.ind, ]) - ds[-train.ind, 2])^2)
})
# add legend to see which color corresponds to which line
par(c(1,1), xpd=TRUE)
legend("topright", inset=c(-0.2,0), legend = paste("d =", deg), lty = 1, col = colors)
?legend
library(splines)
fit = smooth.spline(Expend, Outstate, cv=TRUE)
library(splines)
#choose df
fit = smooth.spline(Expend, Outstate, cv=TRUE)
library(splines)
#choose df
fit = smooth.spline(Expend, Outstate, cv=TRUE, data=college.train)
library(splines)
#choose df
fit = smooth.spline(college.train$Expend, college.train$Outstate, cv=TRUE)
fit$df
plot(college.train$Expend, college.train$Outstate, col = "darkgrey", main="Smoothing spline")
fit = smooth.spline(college.train$Expend, college.train$Outstate, cv=TRUE)
fit$df #choose df from CV
fit2 = smooth.spline(college.train$Expend, college.train$Outstate, df=4.6)
lines(fit2, col="red", lwd=2)
plot(college.train$Expend, college.train$Outstate, col = "darkgrey", main="Smoothing spline")
fit = smooth.spline(college.train$Expend, college.train$Outstate, cv=TRUE)
fit$df #choose df from CV
fit2 = smooth.spline(college.train$Expend, college.train$Outstate, df=4.6)
lines(fit, col="red", lwd=2)
library(splines)
plot(college.train$Expend, college.train$Outstate, col = "darkgrey", main="Smoothing spline")
fit = smooth.spline(college.train$Expend, college.train$Outstate, cv=TRUE)
fit$df #choose df from CV
lines(fit, col="red", lwd=2)
legend("topright", legend=c("4.6 DF"), lty=1, lwd=2, cex=.8)
library(splines)
plot(college.train$Expend, college.train$Outstate, col = "darkgrey", main="Smoothing spline")
fit = smooth.spline(college.train$Expend, college.train$Outstate, cv=TRUE)
fit$df #choose df from CV
lines(fit, col="red", lwd=2)
legend("topright", legend=c("4.6 DF"), color="red", lty=1, lwd=2, cex=.8)
library(splines)
plot(college.train$Expend, college.train$Outstate, col = "darkgrey", main="Smoothing spline")
fit = smooth.spline(college.train$Expend, college.train$Outstate, cv=TRUE)
fit$df #choose df from CV
lines(fit, col="red", lwd=2)
legend("topright", legend=c("4.6 DF"), col="red", lty=1, lwd=2, cex=.8)
#make a dataframe
ds = College[c("Terminal", "Outstate")]
n = nrow(ds)
# chosen degrees
deg = 1:10
# plot of training data
plot(ds[train.ind, ], col = "darkgrey", main = "Polynomial regression")
colors = rainbow(length(deg))
# iterate over all degrees (1:4) - could also use a for-loop here
MSE = sapply(deg, function(d) {
# fit model with this degree
mod = lm(Outstate ~ poly(Terminal, d), ds[train.ind, ])
# add lines to the plot
lines(cbind(ds[train.ind, 1], mod$fit)[order(ds[train.ind, 1]), ], col = colors[d])
# calculate mean MSE - this is returned in the MSE variable
mean((predict(mod, ds[-train.ind, ]) - ds[-train.ind, 2])^2)
})
MSE
# add legend to see which color corresponds to which line
par(c(1,1), xpd=TRUE)
legend("topright", inset=c(-0.2,0), legend = paste("d =", deg), lty = 1, col = colors)
MSE
#MSE for polynomial regression models (1-10)
MSE
#MSE for smoothing spline
pred = predict.smooth.spline(fit)
#MSE for polynomial regression models (1-10)
MSE
#MSE for smoothing spline
pred = predict(fit)
MSE2 = mean( (college.train - pred)^2 )
pred
pred$size
length(pred)
#MSE for polynomial regression models (1-10)
MSE
#MSE for smoothing spline
pred = predict(fit)
MSE2 = mean( (college.train$Outstate - pred)^2 )
#MSE for smoothing spline
?predict.smooth.spline
#MSE for polynomial regression models (1-10)
MSE
#MSE for smoothing spline
?predict.smooth.spline
pred = predict.smooth.spline(fit)
library(splines)
plot(college.train$Expend, college.train$Outstate, col = "darkgrey", main="Smoothing spline")
fit = smooth.spline(college.train$Expend, college.train$Outstate, cv=TRUE)
fit$df #choose df from CV
lines(fit, col="red", lwd=2)
legend("topright", legend=c("4.6 DF"), col="red", lty=1, lwd=2, cex=.8)
#MSE for polynomial regression models (1-10)
MSE
#MSE for smoothing spline
?predict.smooth.spline
pred = predict.smooth.spline(fit)
#MSE for polynomial regression models (1-10)
MSE
#MSE for smoothing spline
?predict.smooth.spline
pred = predict(fit)
MSE2 = mean( (college.train$Outstate - pred)^2 )
#MSE for polynomial regression models (1-10)
MSE
#MSE for smoothing spline
?predict.smooth.spline
pred = predict(fit, college.train$Outstate)
MSE2 = mean( (college.train$Outstate - pred)^2 )
#MSE for polynomial regression models (1-10)
MSE
#MSE for smoothing spline
?predict.smooth.spline
pred = predict(fit, college.train)
#MSE for polynomial regression models (1-10)
MSE
#MSE for smoothing spline
?predict.smooth.spline
pred = predict(fit, college.train$Expend)
MSE2 = mean( (college.train$Outstate - pred)^2 )
#MSE for polynomial regression models (1-10)
MSE
#MSE for smoothing spline
?predict.smooth.spline
pred = predict.smooth.spline(fit, newdata=college.train, deriv=2)
#MSE for polynomial regression models (1-10)
MSE
#MSE for smoothing spline
?predict.smooth.spline
pred = predict(fit, newdata=college.train, deriv=2)
MSE2 = mean( (college.train$Outstate - pred)^2 )
#MSE for polynomial regression models (1-10)
MSE
#MSE for smoothing spline
?predict.smooth.spline
pred = predict(fit, newdata=college.train)
MSE2 = mean( (college.train$Outstate - pred)^2 )
#MSE for polynomial regression models (1-10)
MSE
#MSE for smoothing spline
?predict.smooth.spline
pred = predict(fit, newdata=college.train)
MSE2 = mean( (college.train$Outstate - pred)^2 )
library(tree)
tree.mod = tree(Outstate ~ .,college.train)
summary(tree.mod)
plot(tree.mod)
text(tree.mod, pretty = 0)
set.seed(4268)
cv.college = cv.tree(tree.mod)
tree.min = which.min(cv.college$dev)
best = cv.college$size[tree.min]
plot(cv.college$size, cv.college$dev, type = "b")
points(cv.college$size[tree.min], cv.college$dev[tree.min], col = "red", pch = 20)
pr.tree = prune.tree(tree.mod, best = 7)
plot(pr.tree)
text(pr.tree, pretty = 0)
