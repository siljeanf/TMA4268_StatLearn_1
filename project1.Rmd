
---
subtitle: "TMA4268 Statistical Learning V2019"
title: "Compulsory exercise 1: Group 24"
author: "Silje Anfindsen and Clara Panchaud"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
 # html_document
 pdf_document
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3)
```

```{r,echo=FALSE}
#install.packages("knitr") #probably already installed
#install.packages("rmarkdown") #probably already installed
#install.packages("ggplot2") #plotting with ggplot
#install.packages("ggfortify")  
#install.packages("MASS")  
#install.packages("dplyr")  
library(knitr)
library(rmarkdown)
library(ggplot2)
library(ggfortify)
library(MASS)
library(dplyr)
```



# Problem 1


## a)

The expected test mean squared error (MSE) at $x_0$ is 

$$E[(y_0-\hat{f}(x_0))^2]$$

for $y_0=f(x_0)+\epsilon$ where f is the true regression function and $\epsilon$ the error term.

## b)
Now we can derive this expression into three terms.




$$
\begin{aligned}
E[(y_0-\hat{f}(x_0))^2]&=  &&\text{definition of y0}\\ 
E[(f(x_0)+\epsilon +\hat{f}(x_0))^2]&=  &&\text{by linearity of the expectation}\\
E[f(x_0)^2]+E[\epsilon^2]+E[\hat{f}(x_0)^2]-2E[f(x_0)\hat{f}(x_0)]+0&=  &&\text{using the definition of variance}\\
f(x_0)^2+Var(\epsilon)+Var(\hat{f}(x_0)) +E[\hat{f}(x_0)]^2-2f(x_0)E[\hat{f}(x_0)]&= \\ \underbrace{\text{Var}(\varepsilon)}_{\text{Irreducible error}}+ + \underbrace{\text{Var}(\hat{f}(x_0))}_{\text{Variance of prediction}} + \underbrace{\left( f(x_0) - \text{E}[\hat{f}(x_0)] \right)^2}_{\text{Squared bias}}
\end{aligned}
$$


## c)
We can interpret the three terms as explained below.<br/>
Irreducible error cannot be reduced regardless how well the method fits the data.<br/>
Variance of prediction relates the amount $\hat{f}(x_0))$ is expected to change for different training data.<br/>
Squared bias estimates how much the prediction differs from the true mean.

## d)
True, False, False, True

## e)
True, False, True, False

## f)
(ii)

## g)
C

# Problem 2

In this problem we will work with the earthworm dataset. Let's start by loading it and having a first look.

```{r, eval=TRUE}
id <- "1nLen1ckdnX4P9n8ShZeU7zbXpLc7qiwt" # google file ID
d.worm <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
attach(d.worm)
head(d.worm)
```


```{r}
str(d.worm)
```


## a)
There are 143 observations of 5 variables, so 143 rows and 5 columns.

The qualitative variables are $Gattung$,$FANGDATUM$ and $Nummer$. We see that the first two are indeed encoded as factors, and $Nummer$ is just the worm-specific ID so it does not have a relevant numerical value. The two remaining ones, $GEWICHT$ and $MAGENUMF$ are quantitative.

## b) 

We want to make a scatterplot of the weight against the circumfence of stomach for each of the three species. Since we want to fit a linear regression, we would like to find a linear relationships between the weight and the circumference.

```{r}
ggplot(d.worm,aes(x=MAGENUMF ,y=GEWICHT ,colour= Gattung)) + geom_point() + theme_bw()
```

The relationship does not look linear, it seems like it has a quadratic shape suggesting $GEWICHT=MAGENUMF^2$. We therefore decided to try again by plotting $MAGENUMF$ versus $\sqrt{GEWICHT}$.

```{r}
ggplot(d.worm,aes(x=MAGENUMF ,y= sqrt(GEWICHT) ,colour= Gattung)) + geom_point() + theme_bw()
```

It now seems like the relationship is linear.

## c)
We will now fit a regression model that predicts the weight given the circumference of the stomach and the species. We will use the transformed version of the variable $GEWICHT$ found in b).
```{r}
model<-lm(sqrt(GEWICHT)~MAGENUMF+Gattung,data=d.worm)
summary(model)
```
We find three different regression models, one for each species. They are specified as follows:
$$ 
\begin{aligned}
Lumbricus:& \sqrt{GEWICHT}=-0.235+0.392*MAGENUMF \\
Nicodrilus :& \sqrt{GEWICHT}=-0.235+ 0.009+0.392*MAGENUMF=-0.226+0.392*MAGENUMF\\
Octolasion :& \sqrt{GEWICHT}=-0.235-0.082+0.392*MAGENUMF =-0.317+0.392*MAGENUMF\\
\end{aligned}
$$

Since $Gattung$ is a predictor with three levels, we need to test for H0: $\beta_2=\beta_3=0$ in order to determine if it is a relevant predictor.
This requires an F-test and is done by the anova function below.
```{r}
anova(model)
```

This would suggest that $Gattung$ is not a relevant predictor since we have a pretty high p-value.


## d)

We will now test if including an interaction between the species and MAGENUMF would be relevant to predict the weight of a worm. We therefore fit a model including the interatction term. 

```{r}
model_2<-lm(sqrt(GEWICHT)~MAGENUMF*Gattung,data=d.worm)
summary(model_2)
```


We test similarly as in c).
```{r}
anova(model_2)
```

The significance level can usually be decided, but we will take the usual $0.05$ level. Then we can conclude that there is no evidence that the interaction term is irrelevant and we would keep the interaction in our model.

## e)

We will now carry out a residual analysis for the model without interaction term. Recall that $\epsilon_i = y_i - \hat y_i \sim N(0,\sigma^2)$. We want to verify the assumptions of the linear model, which are: 

1. The expected value of $\epsilon_i$ is 0: $E(\epsilon_i)=0$.
2. All $\epsilon_i$ have the same variance: $Var(\epsilon_i)=\sigma^2$.
3. The $\epsilon_i$ are normally distributed.
4. The $\epsilon_i$ are independent of each other.

```{r}
library(ggfortify)
autoplot(model,smooth.colour = NA)
```

Looking at the Residual vs. Fitted plot there are no presence of a pattern between the residuals $e_i = y_i - \hat y_i$ and the predicted values $\hat y_i$. This helps us check assumptions 1 and 2. We don´t see a particular pattern so it seems fine. We don´t see any trend in the Scale-Location plot either, reinforcing that the homoscedasticity assumption should be correct. 
 The QQ-plot does not show any evidence against assumption 3.
 The leverage plot is not so relevant here since we are looking at a variable encoded as a factor. The observation 07 seems to be an outlier, it would be good to pay attention to it in the future.



In conclusion, according to those plots we have no evidence against the assumptions of the linear model so it seems like a good choice.



We will make a residual plot for the model without any variable transformations.

```{r}
library(ggfortify)
simple_model<-lm(GEWICHT~MAGENUMF+Gattung, data=d.worm)
autoplot(simple_model)
```

The difference is mostly in the Residuals vs. Fitted and in the Scale-Location plot. We can see some patterns in both so we decided to add a smoothing line that makes it even clearer. This would suggest that at least the homoscedasticity assumption is violated and a linear model on this data is not appropriate. 


## f)
Residual plots are a usefull tool to identify if the assumptions of a linear regression are fulfilled. If they are not, a linear model will not be appropriate and any predictions or inference made from it will be misleading and untrustworthy. The residual plots are useful when we have many covariates because then it is harder to plot them all against each other and identify patterns like we did in this problem.


If the residual plots indicates that some assumptions are violated, we can look closer at the data and try to identify non-linear transformations of the variables that can be used instead.



## g)
False,False,False, True



# Problem 3



## a)

We will create a logistic regression model where the probability for player 1 to win has the form:

$$P(Y_i = 1| {\bf X}={\boldsymbol{x}}_i) = p_i = \frac{e^{\beta_0 + \beta_1x_{i1} + \beta_2 x_{i2} + \beta_3x_{i3} + \beta_4 x_{i4}}}{ 1+ e^{\beta_0 + \beta_1x_{i1} + \beta_2x_{i2}+ \beta_3x_{i3} + \beta_4 x_{i4}}} \ $$

where $x_{i1}$ is the number of aces for player 1 in match $i$, $x_{i2}$ is the number of aces for player 2 in match $i$, and $x_{i3}$ and $x_{i4}$ are the number of unforced errors commited by player 1 and 2 in match $i$. $Y_i=0$ represents player 1 loosing match $i$, $Y_i=1$ represents player 1 winning match $i$.

Let $\alpha = \beta_0 + \beta_1x_{i1} + \beta_2 x_{i2} + \beta_3x_{i3} + \beta_4 x_{i4}$

$$logit(p_i) = log (\frac{p_i}{1-p_i}) = log\Bigg(\frac{\frac{e^\alpha}{1+e^\alpha}}{1-\frac{e^\alpha}{1+e^\alpha}}\Bigg) = log\Bigg(\frac{\frac{e^\alpha}{1+e^\alpha}}{\frac{1+e^\alpha-e^\alpha}{1+e^\alpha}}\Bigg) = log(e^\alpha) = \alpha = \beta_0 + \beta_1x_{i1} + \beta_2 x_{i2} + \beta_3x_{i3} + \beta_4 x_{i4}$$


## b)


Taking the exponential of the formula in a) we get the following formula for the odds:

$$
\frac{p_i}{1-p_i}=\frac{P(Y_i=1|X=x_i)}{P(Y_i=0|X=x_i)}=e^{\beta_0}e^{\beta_1x_{i1}}...e^{\beta_4x_{i4}}
$$
To interpret $\beta_1$ we think about what would happen if we increased $x_{i1}$ by 1 while the other covariates remain the same. We do this by looking at the following ratio of the odds $$\frac{odds(Y_i|X_1=x_{i1}+1)}{odds(Y_i|X_1=x_{i1})}=e^{\beta_1}$$
We see that the odds ratio is changed by a factor $e^{\beta_{1}}$.Therefore, if the number of aces of player 1 increases by 1, this will change his odds ratio of winning by a factor $e^{\beta_1}$. Since more aces is a good thing to have in a tennis match, we expect $\beta_1$ to take a positive value. Let's now fit the model to see.
```{r}
# read file
id <- "1GNbIhjdhuwPOBr0Qz82JMkdjUVBuSoZd"
tennis <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", 
    id), header = T)
r.tennis = glm(Result ~ ACE.1 + ACE.2 + UFE.1 + UFE.2, data = tennis, family = "binomial")
summary(r.tennis)
```
We get $\beta_1=0.363$ which is positive as expected.


## c)
Now we will reduce the number of covariates. The vector of covariates is now $\textbf{x}=(x_1,x_2)$ where $x_1$ denotes the difference between aces perfored by player 1 and player 2, while x2 denotes the difference in unforced errors made by the players.


We divide the data into a training and a test set.
```{r}
# The new variables
tennis$ACEdiff = tennis$ACE.1 - tennis$ACE.2
tennis$UFEdiff = tennis$UFE.1 - tennis$UFE.2

# divide into test and train set
n = dim(tennis)[1]
n2 = n/2
set.seed(1234)
train = sample(c(1:n), replace = F)[1:n2]
tennisTest = tennis[-train, ]
tennisTrain = tennis[train, ]
```


Now we can fit a logistic regression model on the training set.

```{r}
r2.tennis = glm(Result ~ ACEdiff + UFEdiff, data = tennisTrain, family = "binomial")
summary(r2.tennis)

```

With the new variables that we defined, the model now has the following form:

$$
logit(p)= \beta_0 + \beta_1x_1+\beta_2x_2
$$
We can solve for $x_2$ which gives
$$x_2=\frac{logit(p)-\beta_0-\beta_1x_1}{\beta_2}=a+bx_1$$
where $a=\frac{logit(p)-\beta_0}{\beta_2}$ and $b=\frac{-\beta_1}{\beta_2}$. Since we want to use a 0.5 cutoff as decision rule, we replace p by 0.5 in those formulas to find the a and b that define our class boundary.
```{r}

#define logit 
logit <-function(x) {return(log(x/(1-x)))}
#find a and b
a = (logit(0.5)-r2.tennis$coefficients[1])/(r2.tennis$coefficients[3])
b = -r2.tennis$coefficients[2]/r2.tennis$coefficients[3]
```

We plot the training observations with the boundary line.
```{r}
ggplot(r2.tennis,aes(x=ACEdiff,y=UFEdiff ,color=Result)) + geom_point() + theme_bw() + geom_abline(slope=b, intercept=a)
```


It seems like the boundary line separates the two classes quite well on the training set. Now we can predict on the test set and look at the confusion table.

```{r}
probs = predict(r2.tennis, tennisTest, type = "response")
for (i in 1:length(probs)){
  if (probs[i]<0.5){
    probs[i]=0
  }
  else{probs[i]=1}
}

t<-table(tennisTest$Result,probs)
colnames(t)=c("Predicted Loss","Predicted Win")
rownames(t)=c("Actual Loss","Actual Win")
t
```

We will calculate the sensitivity and specificity in order to understand this table better.

$$Sensitivity=\frac{True \ Positive }{Actual \ Positive}$$
$$Specificity=\frac{True \ Negative }{Actual \ Negative}$$

```{r}
Sensitivity<-(t[2,2]/(t[2,2]+t[2,1]))
Specificity<-(t[1,1]/(t[1,1]+t[1,2]))
print(c(Sensitivity,Specificity))
```

Since a high sensitivity and specificity yields a good classification rule, we are happy with the obtained values.


## d)
First of all, $\pi_k=P(Y=k)$, k=0,1, are the prior probabilities. Recall that $Y=0$ denotes player 1 loosing the match. For k=0 $\pi_0$ would then be estimated by $\frac{\#loss}{\#observations}$ while for k=1 this would be $\frac{\#victories}{\#observations}$.

The function $f_k(\textbf{x})=P(\textbf{X}=\textbf{x}|Y=k)$ is the probability to obtain a certain $\textbf{x}$, given which player won the game. It follows a multivariate normal distribution with a class specific mean $\mathbf{\mu_k}$ and a common covariance matrix $\mathbf{\Sigma}$ that we will discuss now.

The mean $\mathbf{\mu_k}=(\mu_{k1},\mu_{k2})$ for k=0,1, is estimated as the average of all training observations $(x_1,x_2)$ from the kth class. For example $\mu_{01}$ is calculating by averaging the differences in aces performed by the players, over the games where player 2 won.

On the other hand the covariance matrix is common to both classes. In order to do this the covariance matrix is estimated for each class by a weighted average of the sample variances. Then a pooled version is calculated by combining the two class specific covariance matrices. 


## e)

We will now look at the decision boundary for LDA between the two classes. Start with inserting the expression for each class distribution into Bayes formula to obtain the posterior probability $p_k(x)= P(\textbf{Y}=k|\textbf{X}=x)$.

$$
\begin{aligned}
P(\textbf{Y}=0|\textbf{X}=x)& =P(\textbf{Y}=1|\textbf{X}=x) \\
\frac{\pi_0f_0(x}{\sum_{i=1}^{2} \pi_if_i(x)}&=\frac{\pi_1f_1(x}{\sum_{i=1}^{2} \pi_if_i(x)}\\
\pi_0f_0(x) &= \pi_1f_1(x)\\
\pi_0 e^{-\frac{1}{2}(x-\mu_0)^T\Sigma(x-\mu_0)}& = \pi_1e^{-\frac{1}{2}(x-\mu_1)^T\Sigma(x-\mu_1)}\\
log(\pi_0) - \frac{1}{2}(x-\mu_0)^T\Sigma(x-\mu_0))& = log(\pi_1) - \frac{1}{2}(x-\mu_1)^T\Sigma(x-\mu_1))\\
log(\pi_0) - \frac{1}{2}\Big(x^T\Sigma^{-1}x-\mu_0^T\Sigma^{-1}x+\mu_0^T\Sigma^{-1}\mu_0-x^T\Sigma^{-1}\mu_0\Big)& = log(\pi_1)-\frac{1}{2}\Big(x^T\Sigma^{-1}x-\mu_1^T\Sigma^{-1}x+\mu_1^T\Sigma^{-1}\mu_1-x^T\Sigma^{-1}\mu_1\Big)\\
log(\pi_0) + x^T \Sigma^{-1}\mu_0 - \frac{1}{2}\mu_0\Sigma^{-1}\mu_0&= log(\pi_1) + x^T \Sigma^{-1}\mu_1 - \frac{1}{2}\mu_1\Sigma^{-1}\mu_1\\
\delta_0(x)& = \delta_1(x)
\end{aligned}
$$
Remark:
$\Big(\mu_0^T\Sigma^{-1}x\Big)^T = x^T\Sigma^{-1}\mu_0$. This is true because we know $\Sigma$ is a positive semidefinitt matrix, where $\Sigma^{-1} = (\Sigma^{-1})^T = (\Sigma^T)^{-1} = \Sigma^{-1}$.

We have now found the discriminant score $\delta_k(x)$. We will assign an observation to class $k$ when $\delta_k(x)$ is largest. In other words we use the rule to classify to class 1 for an observation with covariates $x$ if $\hat{P}(\textbf{Y}=1 \mid \textbf{x})>0.5$. We will now find the formula for the boundary between the classes $ax_1+bx_2+c=0$. 

$$
\begin{aligned}
\delta_0(x)& = \delta_1(x)\\
log(\pi_0) + x^T \Sigma^{-1}\mu_0 - \frac{1}{2}\mu_0\Sigma^{-1}\mu_0&= log(\pi_1) + x^T \Sigma^{-1}\mu_1 - \frac{1}{2}\mu_1\Sigma^{-1}\mu_1\\
\end{aligned}
$$

We do the matrix multiplication of the terms above in order to find an expression for a, b and c. 
Let $x=[x_1,x_2]^T$, $\mu_k = [\mu_{k1},\mu_{k2}]$, and

$$
\Sigma^{-1} = \Bigg[
 \begin{matrix}
  \sigma_{11} & \sigma_{21} \\
  \sigma_{21} & \sigma_{22} \\
 \end{matrix}
\Bigg]
$$
We are now ready to define the constants a,b and c.
$$
\begin{aligned}
 a &= \sigma_{11}(\mu_{01} - \mu_{11}) + \sigma_{12}(\mu_{02}-\mu_{12})\\
 b &= \sigma_{21}(\mu_{01} - \mu_{11}) + \sigma_{22}(\mu_{02}-\mu_{12})\\
 c &= -\frac{1}{2}\mu_0^T\Sigma^{-1}\mu_0 + \frac{1}{2}\mu_1^T\Sigma^{-1}\mu_1 + log\Big(\frac{\pi_0}{\pi_1}\Big)
\end{aligned}
$$
We rearrange the forumla for the boundary in the form of a linear function.

$$x_2 = -\frac{ax_1+c}{b}$$
We will now use R for the calculations of a,b and c in order to plot the boundary line later.



```{r}
##Start by estimating mu,pi and sigma
##Recall Y=0 means player 1 loosing the match
tennisTrain_loss <- tennisTrain[tennisTrain$Result==0,] #player 1 loose
tennisTrain_win <- tennisTrain[tennisTrain$Result==1,] #player 1 win

## mu_k is the mean vector consisting of ACEdiff and UFEdiff for Y=0 and Y=1
mu_0 <- c(mean(tennisTrain_loss$ACEdiff), mean(tennisTrain_win$UFEdiff)) #player 1 loose
mu_1 <- c(mean(tennisTrain_win$ACEdiff), mean(tennisTrain_loss$UFEdiff)) #player 1 win

##pi_k is the lost/won matches over total number of matches
pi_0 <- nrow(tennisTrain_loss)/nrow(tennisTrain)
pi_1 <- nrow(tennisTrain_win)/nrow(tennisTrain)

## First find covariance matrices for the each class
sigma_0 <- cov(tennisTrain_loss[,c("ACEdiff", "UFEdiff")])
sigma_1 <- cov(tennisTrain_win[,c("ACEdiff", "UFEdiff")])

## pooled version of covariance matrices
sigma <- (((nrow(tennisTrain_loss) - 1)) * 
            sigma_0 + (nrow(tennisTrain_win) - 1) * sigma_1) / (nrow(tennis) - 2)

## inverse of the covariance matrix
sigma.inv <- solve(sigma)

##We can now solve for a,b and c
a = sigma.inv[1,1]*(mu_0[1]-mu_1[1])+sigma.inv[1,2]*(mu_0[2]-mu_1[2])
b = sigma.inv[2,1]*(mu_0[1]-mu_1[1])+sigma.inv[2,2]*(mu_0[2]-mu_1[2])
c = -0.5*t(mu_0)%*%sigma.inv%*%mu_0+0.5*t(mu_1)%*%sigma.inv%*%mu_1 + log(pi_0/pi_1)

```


We will now make a plot with the training observations and the class boundary, where we also add the test observations to the plot.

```{r}
ggplot(r2.tennis,aes(x=ACEdiff,y=UFEdiff ,color=Result)) + geom_point() +geom_point(data=tennisTest,shape="triangle")+ theme_bw() + geom_abline(slope=-a/b, intercept=-c/b)

```




## f)
For this task we perform LDA on the training data.
```{r}
LDA=lda(Result~ACEdiff+UFEdiff,data=tennisTrain)
```
We can then classify the results of the test set. We make a confusion table to look at our results.
```{r}
pred <- predict(LDA, tennisTest)$class
table2<-table(tennisTest$Result,pred)
colnames(table2)=c("Predicted Loss","Predicted Win")
rownames(table2)=c("Actual Loss","Actual Win")
table2
```


```{r}
Sensitivity<-(table2[2,2]/(table2[2,2]+table2[2,1]))
Specificity<-(table2[1,1]/(table2[1,1]+table2[1,2]))
print(c(Sensitivity,Specificity))
```
The values are pretty high which is good.

## g)
We will now use QDA on the training set and make the confusion table for the test set. The difference between LDA and QDA is that in LDA we had a common covariance matrix while in QDA the covariance matrix is specific to each class. Therefore in our model we have two different covariance matrices.

```{r}
QDA=qda(Result~ACEdiff+UFEdiff,data=tennisTrain)
pred <- predict(QDA, tennisTest)$class
table3<-table(tennisTest$Result,pred)
colnames(table3)=c("Predicted Loss","Predicted Win")
rownames(table3)=c("Actual Loss","Actual Win")
table3
```

```{r}
Sensitivity<-(table3[2,2]/(table3[2,2]+table3[2,1]))
Specificity<-(table3[1,1]/(table3[1,1]+table3[1,2]))
print(c(Sensitivity,Specificity))
```
The sensitivity is exactly the same as in LDA but the specificity is now a bit lower.

## h)


# Problem 4

## a)

The set of possible values for K is ${1,2,3,...,N_0}$.

To perform 10-fold cross validation, we split the training data into 10 (more or less) equal parts. We fit the model on 9 of the parts and validate it on the remaining one. This is done 10 times, where each part take the role of the validation part once.

Each time we fit the model, we calculate the MSE on the validation set as we had explained in Problem 1 b). This yields $(MSE_1,MSE_2,...,MSE_{10})$. Then we can find the validation error using the following formula:

$$\text{CV}_{10} = \frac1{10} \sum_{i=1}^{10} \text{MSE}_i \ .$$

## b)
True, True, False, False

## c)
```{r}
id <- "1I6dk1fA4ujBjZPo3Xj8pIfnzIa94WKcy"  # google file ID
d.chd <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", 
    id))
head(d.chd)
```
```{r}
model<-glm(chd ~ sbp+sex, data = d.chd, family = "binomial")
summary(model)
```

```{r}
new_data=data.frame(sbp = 140,
                     sex=1)
predict(model,new_data,type="response")

```
The estimated probability of chd for a male with a sbp=140 in the given dataset is 0.383.

## d)

```{r}
n=nrow(d.chd)
new_data=data.frame(sbp = 140,
                     sex=1)

estimated_prob = c()
beta_0=c()
B = 1000
for (i in 1:B) {
   index<- sample(n, n, replace = T)
   model<-glm(chd ~ sbp+sex, data = d.chd, subset = index,     family = "binomial")
   estimated_prob[i]=predict(model,new_data,type="response")
   beta_0[i]=model$coefficients[1]
}

```


We derive the standard error for the set of estimated probabilities.

```{r}
standard_error<-sd(estimated_prob)
```
Our estimation $\hat{p}$ of the probability is the mean of the estimated probabilities from the bootstrap sample.
Its $95\%$ confidence interval is found by the following formula:

$$\hat{p}\pm 1.96\cdot SD(\hat{p})$$
```{r}
c(mean(estimated_prob) - 1.96 * sd(estimated_prob), mean(estimated_prob) + 1.96 * sd(estimated_prob))
```

This means that the true probability of chd for a male with sbp=140 would be found in this interval with probability 0.95. 

# References

James, G., D. Witten, T. Hastie, and R. Tibshirani. 2013. An Introduction to Statistical Learning with Applications in R. New York: Springer.




